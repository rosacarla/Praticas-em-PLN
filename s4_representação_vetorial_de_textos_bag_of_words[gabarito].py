# -*- coding: utf-8 -*-
"""S4-Representação vetorial de textos - Bag of words[GABARITO].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_pWp-Lv77pQNkbf9ivvfn6uiKELUIo8Z

# GABARITO
# Representação vetorial de textos – *Bag of words*
## Processamento de Linguagem Natural
Hoje trabalharemos com um assunto essencial ao PLN moderno, a representação vetorial de textos. Nesta aula você realizará atividades práticas relacionadas a técnica chamada  **Bag of words (BoW)**, visando:
1. Entender o modelo Bag of Words
2. Vetorizar textos utilizando a função `CountVectorizer` da biblioteca `scikit-learn`
3. Compreender as limitações do modelo e como ele pode ser utilizado em tarefas de PLN

### **O que é Bag of Words?**

É uma técnica de PLN na qual transformamos textos em **vetores numéricos** para **extrair características do texto**. Tais características podem ser interpretadas por diversos algoritmos, incluindo (principalmente) os de *Machine Learning*.

Apenas dois passos são necessários no algoritmo de BoW:
1.   Determinar o vocabulário do(s) texto(s)
2.   Realizar contagem do termos (frequência das palavras)

Imagine um corpus com os seguintes documentos:

*   *O menino correu*
*   *O menino correu do cão*
*   *O menino com o cão*

#### 1) Determinar vocabulário
Para determinar o vocabulário, basta definirmos uma lista com todas palavras contidas em nosso corpus.
As palavras encontradas nos documentos acima são: `o`, `menino`, `correu`, `do`, `cão` e `com`

#### 2) Contagem das palavras
Nesta etapa devemos contar quantas vezes cada palavra do vocabulário aparece em cada documento/texto, e criamos um vetor com as quantidades computadas.

![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1wDFX5RknqY8eBBaeurIqPK5KgwzjAKfd)

Assim são gerados vetores individuais de cada documento:

*   *O menino correu*: `[1, 1, 1, 0, 0, 0]`
*   *O menino correu do cão*: `[1, 1, 1, 1, 1, 0]`
*   *O menino com o cão*: `[2, 1, 0, 0, 1, 0]`

> **SACO DE PALAVRAS?** A técnica tem esse nome pois perde-se toda informação contextual do texto, ou seja, onde cada palavra apareceu em cada documento, como se pegássemos todas palavras e colocássemos dentro de um saco!

> A matriz com as frequências das palavras também é chamada de **MATRIZ TERMO-DOCUMENTO**

**Mas, qual a ideia por trás do BoW?**

O BoW segue a ideia de que **documentos semelhantes terão contagens de palavras semelhantes** entre si. Em outras palavras, quanto mais semelhantes forem as palavras em dois documentos, mais semelhantes poderão ser os documentos.
Além disso, ao definir a matriz termo-documento, intui-se que **palavras com alta ocorrência em um documento, sejam importantes a ele**, ou seja, devem estar entre os temas centrais do texto.

**EXEMPLO**: Imagine os vetores a seguir:

![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1dKga70Q9l3IRtW28TBqTcn8c_0kEQcrS)

O vetor do documento 1 e 2 são similares (assim como seus textos). Já o vetor do documento 3 se difere completamente.

### **Utilizando o scikit-learn para calcular o BoW ou matriz termo-documento**
Apesar de ser uma técnica simples de se implementar, não há necessidade, pois ela já é implementada dentro da biblioteca `scikit-learn` sob o nome de `CountVectorizer`
"""

# Exemplo de corpus
corpus = [
            "o menino correu.",
            "o menino correu do cão.",
            "O Menino com o Cão."
]

# corpus = [
#             "gatos são bonitos",
#             "gatos são lindos",
#             "tomate é fruta"
# ]

# Importa funcionalidade de BoW
from sklearn.feature_extraction.text import CountVectorizer

# Cria instância de CountVectorizer
vect = CountVectorizer()

# Transforma o corpus em vetores numéricos (BoW)
X = vect.fit_transform(corpus)
X

# Imprime a ordem de cada coluna
print(vect.get_feature_names())

# Imprime vetores (BoW)
print(X.toarray())

"""> **ATENÇÃO**: O CountVectorizer já transforma as palavras em `lowercase` por padrão, ignora pontuação e coloca as palavras em ordem alfabética nos vetores. Além disso ignora palavras que tenham frequência abaixo ou acima dos parâmetros `min_df` e `max_df`.

É possível também **vetorizar N-Grams** do corpus usando o CountVectorizer, sem necessidade de usar alguma função extra. Geralmente o fazemos para obter mais **contexto** do texto.
"""

# Cria instância de CountVectorizer
# Apenas 2-grams serão gerados
vect = CountVectorizer( ngram_range=(2,2) )

# Transforma o corpus em vetores numéricos (BoW)
X = vect.fit_transform(corpus)
X

# Imprime a ordem de cada coluna
print(vect.get_feature_names())

# Imprime vetores (BoW)
print(X.toarray())

"""> **DICA**: Vale a pena olhar a [documentação](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do CountVectorizer, pois existem diversos parâmetros úteis que podemos utilizar.

### **Limitações do BoW**
Apesar das ideias de que **documentos semelhantes terão contagens de palavras semelhantes** entre si (ao aprender Classificação de Textos você poderá ver isso mais detalhadamente) e que **uma palavra com alta frequência em um documento é considerada importante** funcionarem em vários casos, o modelo BoW tem algumas limitações, entre elas:

*   **Peso igual a todas palavras**: o BoW dá um peso igual a todas palavras. Em nosso exemplo palavras como "com" e "do", tem o mesmo peso de "cão" e "menino". Isso não é bom pois palavras mais comuns (artigos, preposições, etc) deveriam ter peso menor, pois são menos discriminantes.
*   **Significado semântico**: a abordagem básica do BOW não considera o significado da palavra no documento. Ignora completamente o contexto em que é usado. A mesma palavra pode ser usada em vários locais com base no contexto ou nas palavras próximas (embora o uso de n-grams possa amenizar um pouco o problema do contexto).
*   **Tamanho do vetor - maldição da dimensionalidade**: para um documento grande, o tamanho do vetor pode ser enorme, resultando em muito tempo de processamento e alto consumo de memória. Pode ser necessário ignorar as palavras com base na relevância do seu caso de uso.

### **Aplicações do BoW**
Ele é útil em qualquer tarefa em que a posição ou informação contextual do texto não é tão importante. Alguns exemplos são:

*   Identificar o autor de um documento (**classificação de textos**)
*   Agrupar documentos por tópicos (**clusterização**)
*   Análise de sentimentos - identificar "positividade"/"negatividade" de um documento (**regressão**)

### **ATIVIDADE PRÁTICA** - Análise de vetorização de corpus
Nesta atividade iremos analisar um pequeno corpus fictício, realizar a vetorização do mesmo, discutir problemas no modelo e eventualmente tentar corrigí-los.
"""

# Um corpus fictício com 8 documentos
corpus = [
          'Um caminhão está descendo rapidamente um morro.',#0
          'O caminhão está rapidamente descendo o morro.',#1
          'O menininho come bananas.',#2
          'O menino comeu banana.',#3
          'Pizza? Portuguesa!',#4
          'Carro? Marea!',#5
          'Qual Seu Nome?',#6
          'qual seu nome?'#7
]

# Código da QUESTÃO 4
import nltk
from nltk import tokenize
nltk.download('punkt')
nltk.download('rslp')
stemmer = nltk.stem.RSLPStemmer()

new_corpus=[' '.join([stemmer.stem(word) for word in tokenize.word_tokenize(text, language='portuguese')]) for text in corpus]

corpus

new_corpus

corpus = new_corpus

"""#### 1) Mostre a quantidade de caracteres e tokens do corpus


> **DICA**: É possível concatenar todas sentenças da lista em uma String apenas usando a função `join()`


"""

todotexto = ' '.join(corpus)
todotexto

import nltk
from nltk import tokenize
nltk.download('punkt')

tokens = tokenize.word_tokenize(todotexto, language='portuguese')

len(todotexto)

len(tokens)

"""#### 2) Mostre a quantidade de tokens únicos do corpus
Ao saber a quantidade de tokens únicos, qual informação importante você pode afirmar a respeito de um possível BoW?
"""

len(set(tokens))

# Após gerar o vetor, podemos mostrar para os alunos pq houve diferença na quantidade de tokens únicos para a dimensão dos vetores
set(tokens)

"""#### 3) Aplique o BoW no corpus e mostre as dimensões, palavras e os valores do vetor resultante"""

from sklearn.feature_extraction.text import CountVectorizer
vect = CountVectorizer()
X = vect.fit_transform(corpus)
X

print(vect.get_feature_names())

print(X.toarray())

"""#### 4) Coloque o nome das colunas e valores do vetor em um `DataFrame` do `pandas`"""

import pandas as pd
results = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())
results

"""#### 5) Ao comparar os vetores resultantes e aplicar a função de Distância de Jaccard nos documentos 2 e 3 verificamos que eles são bem dissimilares, apesar de serem sentenças com valor semântico similar. **Como podemos tornar os vetores do BoW para estes documentos mais semelhantes?**"""

# Imprime vetor do documento 2
print(X[2].toarray())

# Imprime vetor do documento 3
print(X[3].toarray())

d2_set = set(nltk.word_tokenize(corpus[2], language='portuguese'))
d3_set = set(nltk.word_tokenize(corpus[3], language='portuguese'))

#Calcula Jaccard
nltk.jaccard_distance(d2_set, d3_set)

"""Você pode alterar/incluir células nas questões anteriores para resolver essa questão. Depois re-execute as 3 células acima para checar os resultados.

#### 6) Gere um BoW com 2-grams e 3-grams
Olhe a documentação do CountVectorizer para verificar como parametrizar para gerar os dois simultaneamente.
"""



"""## Referências e Material complementar

*   [An Introduction to Bag-of-Words in NLP](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)
*   [A Simple Explanation of the Bag-of-Words Model](https://victorzhou.com/blog/bag-of-words/)
"""