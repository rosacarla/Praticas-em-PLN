# -*- coding: utf-8 -*-
"""S2-Processando textos usando o NLTK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JT88d2XmUVBh9JxDuYSSEU3YCxGYzZVe

># Processamento de textos usando o NLTK
>Instituição: PUCPR  
>Curso: Tecnologia em Inteligência Artifical Aplicada  
>Disciplina: Processamento de Linguagem Natural  
>Professor: Lucas Oliveira   
>Estudante: Carla Edila Silveira  
> Data: 10/04/2024  

> <img src='https://i.postimg.cc/vBVbCxqG/nlp-header.jpg'>
</br>

---  
</br>

## Processamento de Linguagem Natural
<p align='justify'> Nesta atividade você entrará em contato com a biblioteca <b>NLTK</b>, que fornece uma série de funcionalidades essenciais para o processamento de textos e, o mais importante, dá suporte a essas funcionalidades para a língua portuguesa.</p>
<p align='justify'> Além de fazermos um apanhado geral sobre as funcionalidades do NLTK, tentaremos replicar algumas dessas funcionalidades e também utilizá-las na solução de alguns problemas.</p>  

As **expressões regulares** também serão exploradas para complementar algumas funcionalidades.

##**Natural Language Toolkit - NLTK**
<p align='justify'> Como o nome já diz, o NLTK é um toolkit para processamento de linguagem natural. Ele fornece várias funcionalidades para criação de programas de PLN. O NLTK foi desenvolvido com 4 objetivos principais:</p>

<p align='justify'> 1. SIMPLICIDADE: Fornecer um framework intuitivo junto a substanciais blocos de construção, dotando os usuários de um conhecimento prático de PLN sem se prender nas tediosas tarefas de "arrumação da casa", geralmente associadas com o processamento de dados linguísticos anotados.</p>
<p align='justify'> 2. CONSISTÊNCIA: Fornecer um framework unificado com interfaces e estruturas de dados consistentes, e nomes de método facilmente conjecturáveis. </p>
<p align='justify'>3. EXTENSIBILIDADE: Fornecer uma estrutura na qual novos módulos de software possam ser acomodados facilmente, incluindo implementações alternativas a abordagens diversas para uma mesma tarefa.</p>
<p align='justify'> 4. MODULARIDADE: Fornecer componentes que possam ser utilizados independentemente, sem qualquer necessidade de compreender o restante do toolkit. </p>

### **Importa a biblioteca NLTK**
"""

import nltk

"""<p align='jusrify'> O NLTK já vem instalado no Google Colab por padrão, porém, alguns recursos como corpora textuais podem não estar disponíveis. Para tal, você deve explicitamente recomendar seu download usando o comando a seguir: </p>"""

nltk.download()

"""> CONSULTA AO CHATGPT SOBRE CORPORA EM PT  

RESPOSTA: Sim, o NLTK oferece alguns corpora em português que podem ser úteis para diversas tarefas de processamento de linguagem natural. Aqui estão alguns corpora em português que você pode recomendar o download:

1. **Floresta Sintá(c)tica**: Um corpus de árvores de análise sintática para o português brasileiro.

   ```python
   nltk.download('floresta')
   ```

2. **Mac-Morpho**: Um corpus anotado morfossintaticamente, contendo textos em português brasileiro.

   ```python
   nltk.download('mac_morpho')
   ```

3. **Cess_esp**: Corpus de sentenças em espanhol, traduzido para o português, útil para testes de tokenização.

   ```python
   nltk.download('cess_esp')
   ```

4. **HAREM**: Um corpus anotado com entidades nomeadas, útil para reconhecimento de entidades em português.

   ```python
   nltk.download('harem')
   ```

5. **Inforex**: Um corpus de notícias em português.

   ```python
   nltk.download('inforex')
   ```

6. **Stopwords**: Lista de palavras comuns que geralmente são removidas durante a pré-processamento de texto.

   ```python
   nltk.download('stopwords')
   ```

Esses são apenas alguns exemplos de corpora em português disponíveis no NLTK. Dependendo da tarefa específica que você está realizando, pode ser necessário ou útil baixar outros corpora ou recursos.

### **Operações Básicas ou Pré-processamento**
<p align='justify'> As operações a seguir geralmente fazem parte de uma etapa do PLN chamada de <b>Pré-processamento</b>, e já se tornaram triviais a todos programas de PLN, pois preparam o texto bruto para ser realmente processado e "entendido" pela máquina.</p>

*   Tokenização
*   Segmentação de sentenças
*   Normalização
*   Stemming
*   Lematização

#### Tokenização
Serve para separar o texto em *tokens* - que são uma sequência de caracteres com algum significado semântico.

**Principais dificuldades**:
*   "São Paulo" - uma ou duas palavras?
*   "A seleção dos E.U.A. venceu." - Pontuação pode ser considerada quebra de sentença
*   "Tromba d'água"
*   "São João da Boa Vista"
*   "Interação humano-computador"
"""

# Você deve importar o tokenizador da biblioteca NLTK
import nltk
from nltk import tokenize

# Caso não tenha feito o download de todos recursos do NLTK, você pode fazê-lo de maneira individual
nltk.download('punkt')

texto = "Um exemplo de texto para visualizarmos a técnica de tokenização."

# Tokeniza o texto
tokens = tokenize.word_tokenize(texto, language='portuguese')

tokens

"""Quantos caracteres temos?"""

len(texto)

"""Quantos tokens temos?"""

len(tokens)
# O ponto final é considerado um token

"""Quantos tokens únicos nós temos?"""

# Usando funcionalidades básicas do Python
len(set(tokens))

# Usando a biblioteca collections
from collections import Counter

contador = Counter(tokens)

for cont in contador.items():
  print(cont)

""">CONSULTA AO CHATGPT SOBRE A CONTAGEM DE TOKENS ÚNICOS  

RESPOSTA: (...) Vamos analisar os resultados que obtidos:

1. <b>Tokenização</b>: A tokenização incluiu um token adicional, que é o ponto no final da sentença. Isso resultou em uma lista de tokens com 11 elementos.</p>

2. <b>Contagem de tokens únicos com Python básico</b>: Ao usar a função `set(tokens)` para contar os tokens únicos, obteve-se 10. Isso significa que há um token repetido na lista de tokens.

3. **Contagem de tokens únicos com collections**: Ao usar a biblioteca `collections` para contar os tokens únicos, obteve-se uma contagem onde "de" aparece 2 vezes. Cada token é exibido com sua contagem de ocorrência.</p>

<p align='justify'> Pode haver diferença nas contagens de tokens únicos devido à presença do ponto no final da sentença. Se quisermos contar apenas os tokens de texto e ignorar pontuações, devemos remover o ponto antes de contar os tokens únicos. Isso é comumente feito filtrando os tokens para incluir apenas aqueles que são compostos de letras.</p>

Aqui está como você pode fazer isso:

```python
import string

# Remover pontuações
tokens = [token for token in tokens if token not in string.punctuation]

# Contagem de tokens únicos
num_unique_tokens = len(set(tokens))
print("Tokens únicos:", num_unique_tokens)
```

Isso deve fornecer a contagem correta de tokens únicos, desconsiderando a pontuação.
"""

# Mostra os termos mais frequentes
contador.most_common(3)

"""#### Segmentação de sentenças
As regras principais de segmentação de sentenças contam com a divisão a partir de pontuações encontradas no texto ou quebras de linha.

![Exemplo de segmentação de sentença - Dan Jurafsky](https://docs.google.com/uc?export=download&id=15fLctvbOi8_STzO8AOzzW095yA2IvPmK)

"""

from nltk import sent_tokenize

texto = "Definição da sentença 1. Mais uma sentença. Última sentença."

sents = sent_tokenize(texto)
sents

"""#### Stemming
<p align='justify'> Reduz as palavras ao seu <i>stem</i>, retirando o sufixo. Faz com que palavras de mesmo significado semântico (ou similar) sejam escritas da mesma maneira (e.g., correr, correndo, correu). Geralmente o stem não é uma palavra válida.</p>

"""

# Caso não tenha feito o download de todos recursos do NLTK, você pode fazê-lo de maneira individual
nltk.download('rslp')

# Inicia o Stemmer
stemmer = nltk.stem.RSLPStemmer()

print(stemmer.stem("ferro"))
print(stemmer.stem("ferreiro"))

print(stemmer.stem("correr"))
print(stemmer.stem("correu"))

# Define uma função que faz Stemming em todo um texto
def Stemming(texto):
  stemmer = nltk.stem.RSLPStemmer()
  novotexto = []
  for token in texto:
    novotexto.append(stemmer.stem(token.lower()))
  return novotexto

texto1 = "Eu gostei de correr"
texto2 = "Eu gosto de corrida"

# Tokeniza o texto
tokens1 = tokenize.word_tokenize(texto1, language='portuguese')
tokens2 = tokenize.word_tokenize(texto2, language='portuguese')

novotexto1 = Stemming(tokens1)
novotexto2 = Stemming(tokens2)

print(novotexto1)
print(novotexto2)

"""#### Lematização
<p align='justify'> Similar ao processo de Stemming, porém, faz uma análise morfológica completa para identificar e remover os sufixos. Geralmente leva os verbos ao infinitivo e substantivos/adjetivos ao masculino singular. Diferencia-se do Stemming, pois sempre gera uma palavra válida.</p>

<p align='justify'> Esta funcionalidade não é suportada pelo NLTK. <a href='https://lars76.github.io/nlp/lemmatize-portuguese/'>Neste link</a> você pode encontrar alternativas para realizar a lematização em português.</p>

<p align='justify'> Para nossa disciplina, utilizaremos o <i>stemmer</i> e, caso queira saber um pouco mais sobre o impacto dessa decisão, você pode ler <a href='https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html'>este capítulo</a> do livro de <i>Information Retrieval</i> da Universidade de Stanford.</p>

#### Retirada de Stop Words
<p align='justify'>As vezes, é necessário remover as palavras de maior ocorrência no conjunto de textos, pois geralmente elas não agregam grande valor semântico aos textos e não ajudam no processo de selecionar as informações relevantes ao sistema de PLN.</p>

<p align='justify'>Este processo pode ser diferente, de acordo com a tarefa de PLN que você está executando, mas no geral temos duas abordagens: retirar as palavras de maior ocorrência, levando em conta <a href='http://terrierteam.dcs.gla.ac.uk/publications/rtlo_DIRpaper.pdf'>lei de Zipg</a>, ou utilizar uma lista de stop-words pronta para seu idioma. Realizaremos a segunda opção.</p>
"""

# Caso não tenha feito o download de todos recursos do NLTK, você pode fazê-lo de maneira individual
# nltk.download('stopwords')

# O NLTK fornece uma lista de stop words para o idioma português
stopwords = nltk.corpus.stopwords.words('portuguese')
stopwords

# Define uma função que remove as stop words de um texto
def removeStopWords(texto):
    stopwords = nltk.corpus.stopwords.words('portuguese')
    novotexto = []
    for token in texto:
        if token.lower() not in stopwords:
            novotexto.append(token)
    return novotexto

texto = "Quais palavras serão retiradas deste texto? Eu não sei, mas este processo é necessário em alguns momentos."

# Tokeniza o texto
tokens = tokenize.word_tokenize(texto, language='portuguese')

novotexto = removeStopWords(tokens)

print(novotexto)

"""><p align='justify'><b>IMPORTANTE</b>: Em alguns casos, retirar as palavras referentes à negação (i.e., não) pode retirar um significado semântico muito importante do texto. Por exemplo, no texto "<i>O paciente não apresenta sintomas da doença</i>", a negação muda completamente o sentido da frase. Existem alguns outros casos (principalmente quando utilizamos <i>Deep Learning</i>) em que <b>a retirada das stop words pode ser prejudicial ao algoritmo</b>, portanto, sempre teste seus algoritmos com e sem esta opção!</p>  
</br>

![Pré-processamento nem sempre é benéfico](https://docs.google.com/uc?export=download&id=1kx5DedeB1X4AJTWD7Urdqz9HzZ62D5FW)

#### Normalização
Além do Stemming, é possível realizar processos mais específicos de normalização do texto, de acordo com a tarefa.

> <p align='justify'><b>Exemplo 1</b>: Para um algoritmo de Geração de fala, o texto bruto pode estar escrito como "Com os preços de R$ 100,00 para a primeira versão e R$ 10,00 para a segunda". Para o meu algoritmo é mais interessante que o texto seja normalizado para: "Com os preços de cem reais para a primeira versão e dez reais para a segunda."

> <p align='justify'><b>Exemplo 2</b>: Normalizar todas as datas para um padrão único. As formas "18/mai", "dezoito de maio", "18-05" seriam normalizadas para "18/05".</p>

> **Pergunta rápida**: Nos casos acima, qual recurso nos ajudaria a fazer a normalização?

><p align='justify'>Uma maneira simples de normalizar o texto é transformar todas as letras em minúsculas, assim ocorrências escritas de maneira diferente são normalizadas para uma única. "Brasil", "BraSil" e "BRAsil" seriam normalizadas para "brasil".</p>
"""

texto = "Se escreve LOL, LoL ou Lol?"

# Efetua lowercase
texto = texto.lower()

# Tokeniza o texto
tokenize.word_tokenize(texto, language='portuguese')

"""### **A coleção de corpora do NLTK**
O NLTK contém diversos corpora disponíveis, inclusive para língua portuguesa.
> **DEFINIÇÃO**: Um corpus é uma coleção de documentos.

"""

# Importa o corpus dos livros de Machado de Assis
from nltk.corpus import machado

nltk.download('machado')

# Cada arquivo corresponde a um livro
machado.fileids()

machado.readme()

# Obtém o texto de "Memórias Póstumas de Brás Cubas"
texto = machado.raw('romance/marm05.txt')
texto

# Você pode selecionar uma parte específica do texto
texto[9:40]

"""### **Part-of-speech Tagging (POS-Tagging)**
<p align='justify'>Esta também pode ser considerada uma das operações básicas de PLN, que tem por objetivo definir o valor morfológico de cada palavra no texto (e.g., substantivo, adjetivo, verbo, artigo, advérbio).</p>

<p align='justify'>O objetivo da morfologia é estudar a estrutura interna e a variabilidade das palavras em uma língua, como conjugações verbais, plurais, nominalização, etc.</p>

<p align='justify'>Ao contrário dos processos mostrados até agora, este depende do treinamento de um algoritmo supervisionado de <i>Machine Learning</i>, treinado a partir de <b>corpus anotado com as informações morfológicas de cada palavra</b>.  

> ><p align='justify'> <b>DEFINIÇÃO</b>: Um corpus anotado é uma coleção de documentos etiquetada por humanos para identificar determinado valor morfológico, sintático ou semântico do texto.</p>

No caso a seguir, vamos trabalhando com um corpus anotado com informações morfológicas de palavras.

#### Principais dificuldades
As principais dificuldades na realização deste processo são:


* <p align='justify'><b>Ambiguidade</b>: uma mesma palavra pode ter papéis diferentes de acordo com o contexto (e.g., "Ele deu um parecer" - "O verbo parecer")</b>
* <p align='justify'><b>Palavras fora do vocabulário</b>: Quando o corpus não contém alguma palavra, fica difícil para o POS-Tagger "adivinhar" o valor morfológico da palavra. Isso é especialmente comum quando se utiliza um POS-Tagger treinado em domínio em textos de algum domínio específico, por exemplo, utilizar um POS-Tagger treinado em textos jornalísticos para marcação de um texto de prontuários de pacientes.</p>

#### Corpus anotado MacMorpho
<p align='justify'>É um corpus de notícias em português, com mais de um milhão de palavras de textos jornalísticos da Folha de São Paulo. Todo o corpus foi taggeado/etiquetado com os valores morfológicos para cada palavra.</p>
"""

from nltk.corpus import mac_morpho
#nltk.download('mac_morpho')

# Palavras no corpus
mac_morpho.words()

# Sentenças
mac_morpho.sents()

# A palavras e suas tags
mac_morpho.tagged_words()

mac_morpho.tagged_sents()

# Vamos contar a quantidade de tokens
total_tokens = len(mac_morpho.words())

# Exibindo o resultado com divisão das casas de milhar
print("Quantidade de tokens:", "{:,}".format(total_tokens))

# Palavras únicas
total_uniques = len(nltk.FreqDist(mac_morpho.words()))

# Exibindo o resultado com divisão das casas de milhar
print("Quantidade de palavras únicas:", "{:,}".format(total_uniques))

# Palavra mais frequente
nltk.FreqDist(mac_morpho.words()).max()

"""#### Treinamento do POS-Tagging do NLTK
Podemos utilizar um corpus anotado morfologicamente para treinar o POS-Tagger do NLTK
"""

from nltk.corpus import mac_morpho

#Obtém as sentenças taggeadas
tagged_sents = mac_morpho.tagged_sents()

# Divide em duas partes: uma maior para TREINAMENTO e outra menor para TESTE
train_tsents = tagged_sents[100:] # Todas as sentenças após as 100 primeiras
test_tsents = tagged_sents[:100] # Pega todas as sentenças até a centésima

from nltk import DefaultTagger

# Define um tagger padrão, que sempre etiquetará a palavra com "N" = "NOUM" = "SUBSTANTIVO", visto que é a tag que mais ocorre
tagger0 = DefaultTagger("N")
# Avalia a acurácia do POS-Tagger ao etiquetar as sentenças de TESTE
tagger0.accuracy(test_tsents)

# Função evaluate() foi substituída por accuracy() devido à mensagem de erro:
# <ipython-input-35-d73aa04ab757>:6: DeprecationWarning:
# Function evaluate() has been deprecated.  Use accuracy(gold) instead.

from nltk import UnigramTagger

# Define um tagger Unigram (falaremos mais sobre isso depois).
# Este tagger aprende ao ver as sentenças etiquetadas na base de TREINAMENTO.
# Além disso, utiliza o DefaultTagger, caso não saiba o que marcar.
tagger1 = UnigramTagger(train_tsents, backoff=tagger0)
tagger1.accuracy(test_tsents)

from nltk import BigramTagger

# Define um tagger Bigram (falaremos mais sobre isso depois)
tagger2 = BigramTagger(train_tsents, backoff=tagger1)
tagger2.accuracy(test_tsents)

# Existe ainda mais um POS-Tagger no NLTK, o TnT
from nltk.tag import tnt
tnt_pos_tagger = tnt.TnT()
tnt_pos_tagger.train(train_tsents)
tnt_pos_tagger.accuracy(test_tsents)

# Se deseja apenas realizar o POS-Tagging e não avaliar
tagger2.tag(tokenize.word_tokenize(texto, language='portuguese'))

"""Caso queira armazenar o modelo treinado para evitar o re-treinamento veja a seção 5.6 deste [link](https://www.nltk.org/book/ch05.html).

##**ATIVIDADE PRÁTICA**
A seguir, algumas práticas relacionadas às operações básicas de PLN, NLTK e expressões regulares.
"""

# Texto de exemplo
texto = """A morte supersônica
O Concorde era um avião incrível. Inclusive no quesito segurança. Até que, um dia, aconteceu algo que ninguém poderia prever.

Com asas alongadas e viradas para trás, cabine bem estreita e bico fino – que deslizava para baixo durante o pouso, para melhorar a visibilidade do piloto –, o Concorde é um dos aviões mais bonitos de todos os tempos. E um dos mais avançados também.

Desenvolvido nos anos 1960 por um consórcio de empresas francesas e inglesas (que hoje se chama Airbus), ele foi um dos únicos dois aviões supersônicos a operar comercialmente (o outro foi o soviético Tupolev Tu-144). Tinha quatro motores ultrapotentes, que queimavam 25 mil litros de combustível por hora – quase o dobro de um Boeing 747 -, e viajava a 60 mil pés de altitude, 20 mil a mais que os outros aviões. Usava tecnologias revolucionárias, como um sistema que desacelerava a passagem do ar dentro das turbinas (do contrário, elas explodiriam antes que o avião atingisse sua velocidade máxima). Tudo para alcançar a glória de voar a 2.179 km/h, duas vezes a velocidade do som. É o dobro dos aviões comuns, e permitia atravessar o planeta com uma rapidez que até hoje impressiona – ir de Londres a Nova York em três horas, por exemplo.

O Concorde começou a operar em 1976. Uma das primeiras linhas fazia o trajeto Paris-Rio de Janeiro, com dois voos semanais. O público ficou maravilhado com a aeronave, mas voar nela era para poucos: as passagens eram ainda mais caras que as dos voos de primeira classe, e ultrapassavam US$ 10 mil em valores de hoje. Mas manter a frota de Concordes era ainda mais caro, e a British Airways e a Air France, que operavam os aviões, tinham prejuízo com eles. É que, para que os voos dessem lucro, cada avião precisava decolar com pelo menos 90% dos assentos ocupados, o que nem sempre acontecia (a linha brasileira tinha ocupação média de 50%, e por isso foi uma das primeiras a deixarem de existir, em 1982).

Mesmo assim, ele continuou operando comercialmente, com voos ligando a Europa e os EUA, até o dia 25 de julho de 2000. Nesse dia, o Concorde F-BTSC da Air France (um dos únicos 26 a serem produzidos) saiu do aeroporto Charles de Gaulle, em Paris, com destino a Nova York. Levava cem passageiros e nove tripulantes – e, descobriu-se depois, estava sobrecarregado, com 810 kg a mais do que o peso máximo permitido. Na decolagem, aconteceu uma das cenas mais impressionantes da história da aviação. Durante a aceleração inicial, com o Concorde ainda no chão, um dos tanques de combustível explodiu. Os pilotos perceberam, mas já tinham percorrido boa parte da pista – só tinham mais 2 km, espaço insuficiente para frear o avião e abortar o voo com segurança. A única saída era tentar decolar – coisa que eles fizeram, com o Concorde pegando fogo.

As consequências foram dramáticas. Os motores 1 e 2 pifaram, e com isso a aeronave foi se inclinando violentamente para o lado. Para tentar compensar isso, e forçar o Concorde ferido a voar reto, os pilotos desaceleraram os motores 3 e 4, do outro lado. O avião caiu logo depois, nas redondezas do aeroporto. Todos os ocupantes morreram, e também houve quatro vítimas fatais em solo, que foram atingidas por pedaços do avião. O acidente chocou o mundo e devastou a imagem do Concorde, que nunca havia enfrentado um acidente grave e até então era considerado um dos aviões mais seguros da história.

A frota foi inspecionada e voltou a operar comercialmente. Mas aí, em 2001, aconteceram os atentados de 11 de Setembro, que deixaram as pessoas com medo de voar. O Concorde não teve nada a ver com eles, mas também foi afetado pela forte queda na venda de passagens aéreas. E se tornou, de uma vez por todas, economicamente inviável. Foi aposentado – com direito, apenas, a uma curta série de voos de despedida, para os fãs do avião, realizados pela British Airways em 2003.

O verdadeiro culpado

Como um relatório do governo francês constatou depois, o acidente fatal não aconteceu por defeitos no Concorde (nem pelo excesso de peso). Foi por causa de outro avião: um DC-10 da Continental Airlines, que decolou cinco minutos antes. Esse avião teve manutenção malfeita, e por isso soltou um pedaço de metal na pista. Era uma faixa de titânio, com 40 cm de comprimento e 3 cm de largura, que caiu de uma das turbinas. Ninguém percebeu, a peça ficou lá – e o Concorde passou bem por cima dela. Isso rasgou um de seus pneus, que estourou. A onda de choque rompeu um dos tanques de combustível do Concorde, detonando o incêndio que derrubou o avião."""

"""### 1) Usando expressões regulares desenvolva seu próprio tokenizador. Compare o seu resultado com o tokenizador do NLTK.</p>"""

import re

# Tokenizador personalizado usando expressões regulares
def nlp_tokenizador(texto):
    # Expressão regular para separar palavras por espaços em branco, considerando pontuações
    tokens = re.findall(r'\b\w+\b', texto)
    return tokens

# Aplica o tokenizador personalizado
tokens_personalizado = nlp_tokenizador(texto)
print("Tokenizador personalizado:")
print(tokens_personalizado)
print("Total de tokens:", len(tokens_personalizado))

# Usa o tokenizador do NLTK para comparação
nltk.download('punkt')
from nltk.tokenize import word_tokenize

tokens_nltk = word_tokenize(texto)
print("\nTokenizador NLTK:")
print(tokens_nltk)
print("Total de tokens:", len(tokens_nltk))

"""### 2) Mostre o tamanho do texto (em caracteres)"""

# Tamanho do texto em caracteres
tamanho_texto = len(texto)
print("Tamanho do texto em caracteres:", tamanho_texto)

"""### 3) Mostre o tamanho do texto (em tokens)"""

# Tamanho do texto em tokens usando o tokenizador personalizado
tamanho_tokens_personalizado = len(tokens_personalizado)
print("Tamanho do texto em tokens (personalizado):", tamanho_tokens_personalizado)

# Tamanho do texto em tokens usando o tokenizador do NLTK
tamanho_tokens_nltk = len(tokens_nltk)
print("Tamanho do texto em tokens (NLTK):", tamanho_tokens_nltk)

"""### 4) Mostre a quantidade de tokens únicos do texto"""

# Quantidade de tokens únicos usando o tokenizador personalizado
qtde_tokens_personalizado_unicos = len(set(tokens_personalizado))
print("Quantidade de tokens únicos (personalizado):", qtde_tokens_personalizado_unicos)

# Quantidade de tokens únicos usando o tokenizador do NLTK
qtde_tokens_nltk_unicos = len(set(tokens_nltk))
print("Quantidade de tokens únicos (NLTK):", qtde_tokens_nltk_unicos)

"""### 5) Realize o Stemming no texto, e mostre novamente a quantidade de tokens únicos"""

from nltk.stem import SnowballStemmer

# Inicialização do stemmer
stemmer = SnowballStemmer('portuguese')

# Aplicação do stemming em cada token do texto
tokens_stemmed = [stemmer.stem(token) for token in tokens_nltk]

# Quantidade de tokens únicos após o stemming
qtde_tokens_stemmed_unicos = len(set(tokens_stemmed))
print("Quantidade de tokens únicos após stemming:", qtde_tokens_stemmed_unicos)

"""### 6) Realize a normalização para lowercase e mostre novamente a quantidade de tokens únicos"""

# Converte o texto para minúsculas
texto_lower = texto.lower()

# Usa o tokenizador do NLTK para tokenizar o texto em minúsculas
tokens_lower = word_tokenize(texto_lower)

# Quantidade de tokens únicos após a normalização para minúsculas
qtde_tokens_lower_unicos = len(set(tokens_lower))
print("Quantidade de tokens únicos após a normalização para minúsculas:", qtde_tokens_lower_unicos)

"""### 7) Realize a retirada de stop words (utilizando o conjunto do NLTK) e mostre a quantidade de tokens no texto"""

from nltk.corpus import stopwords

# Carrega as stop words para o idioma português
stop_words = set(stopwords.words('portuguese'))

# Remove as stop words do texto
tokens_sem_stopwords = [token for token in tokens_nltk if token.lower() not in stop_words]

# Quantidade de tokens no texto após a remoção das stop words
qtde_tokens_sem_stopwords = len(tokens_sem_stopwords)
print("Quantidade de tokens no texto após a remoção de stop words:", qtde_tokens_sem_stopwords)

"""### 8) Remova todas pontuações do texto (exceto quando estiver sendo usado como separador decimal)"""

# Define expressão regular para remover as pontuações, exceto quando usadas como separador decimal em números
texto_sem_pontuacoes = re.sub(r'(?<!\d)[^\w\s.]+(?!\.)', ' ', texto)

# Exibe o texto sem pontuações
print("Texto sem pontuações (exceto separadores decimais):")
print(texto_sem_pontuacoes)

"""### 9) Encontre todos os valores associados a unidades de medida no texto (e.g., cm, kg)"""

# Define expressão regular para encontrar valores associados a unidades de medida
padrao_unidades = r'\d+(?:\.\d+)?\s*(cm|mm|m|km|g|kg|mg|ml|l|Hz|kHz|MHz|GHz)'

# Encontra todos os valores associados a unidades de medida no texto
valores_unidades = re.findall(padrao_unidades, texto)

# Exibe os valores associados a unidades de medida encontrados
print("Valores associados a unidades de medida no texto:")
print(valores_unidades)

"""### 10) Realize o treinamento do BigramTagger e do TnT tagger usando o corpus Floresta disponível no NLTK. Após o treinamento realize o tageamento do texto."""

from nltk.corpus import floresta

# Carregar o corpus Floresta
corpus_floresta = floresta.tagged_sents()
corpus_floresta

# Obtém as sentenças taggeadas do corpus Floresta
tagged_sents_floresta = floresta.tagged_sents()

# Divide em duas partes: uma maior para TREINAMENTO e outra menor para TESTE
train_floresta = tagged_sents_floresta[100:]  # Todas as sentenças após as 100 primeiras
test_floresta = tagged_sents_floresta[:100]  # Pega todas as sentenças até a centésima

# Mostra as sentenças taggeadas do corpus Floresta
tagged_sents_floresta

# Treinamento do BigramTagger
bigram_tagger_floresta = BigramTagger(train_floresta)

# Avaliação do desempenho do BigramTagger no conjunto de teste
print("Desempenho do BigramTagger no conjunto de teste:", bigram_tagger_floresta.accuracy(test_floresta))

# Treinamento do TnT tagger
tnt_tagger_floresta = tnt.TnT()
tnt_tagger_floresta.train(train_floresta)

# Avaliação do desempenho do TnT tagger no conjunto de teste
print("Desempenho do TnT tagger no conjunto de teste:", tnt_tagger_floresta.accuracy(test_floresta))

# Texto para realizar o tagging
texto = "Esta é uma frase de exemplo para fazer o etiquetar."

# Tokenização do texto
tokens_texto = word_tokenize(texto)
tokens_texto

# Realizar o tagging do texto com o BigramTagger
tags_bigram = bigram_tagger_floresta.tag(tokens_texto)

# Imprime as palavras e suas etiquetas de forma mais legível
print("Tagging do texto com o BigramTagger:")
for word, tag in tags_bigram:
    print(f"{word} - {tag}")

# Realizar o tagging do texto com o TnT tagger
tags_tnt = tnt_tagger_floresta.tag(tokens_texto)

# Imprime as palavras e suas etiquetas de forma mais legível
print("Tagging do texto com o BigramTagger:")
for word, tag in tags_tnt:
    print(f"{word} - {tag}")

"""### 11) Crie uma expressão regular para obter no nome de todas pontifícias univerdades católicas do texto a seguir."""

textoUniv = """A seguir uma lista de todas pontifícias universidades católicas no Brasil.
- Pontifícia Universidade Católica do Rio de Janeiro
- Pontifícia Universidade Católica de São Paulo
- Pontifícia Universidade Católica de Campinas
- Pontifícia Universidade Católica de Minas Gerais
- Pontifícia Universidade Católica do Rio Grande do Sul
- Pontifícia Universidade Católica do Paraná
- Pontifícia Universidade Católica de Goiás

Todas estas universidades são mantidas pela Igreja Católica."""

# Expressão regular para encontrar os nomes das universidades
padrao = r"Pontifícia Universidade Católica [^\n]+"

# Encontrar todas as correspondências
nomes_universidades = re.findall(padrao, textoUniv)

# Imprimir os nomes das universidades
for nome in nomes_universidades:
    print(nome)

"""### 12) Monte no PANDAS uma tabela de frequencia dos TOP 10 tokens"""

import pandas as pd

# Tokenização e remoção de stopwords
tokens = word_tokenize(textoUniv.lower(), language='portuguese')
filtered_tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('portuguese')]

# Contagem de frequência das palavras
word_freq = Counter(filtered_tokens)

# Criação do DataFrame
df = pd.DataFrame(word_freq.most_common(10), columns=['Token', 'Frequência'])

# Imprime a tabela de frequência dos TOP 10 tokens
print(df)

"""### 13) Apresente um gráfico baseado na tabelas acima"""

import matplotlib.pyplot as plt

# Dados da tabela de frequência (exemplo)
tokens = ['universidade', 'católica', 'pontifícia', 'são', 'paulo', 'rio', 'janeiro', 'campinas', 'minas', 'gerais']
frequencia = [7, 7, 7, 2, 1, 1, 1, 1, 1, 1]

# Criar o gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(tokens, frequencia, color='skyblue')
plt.xlabel('Tokens')
plt.ylabel('Frequência')
plt.title('Top 10 Tokens Mais Frequentes')
plt.xticks(rotation=45, ha='right')

# Exibir o gráfico
plt.tight_layout()
plt.show()

"""## Referências e Material complementar

*   [Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit](https://www.nltk.org/book/)
*   [Stanford - Information Retrieval Book - Chapter 2](https://nlp.stanford.edu/IR-book/)
*   [Técnicas de pré-processamento de Texto - Prof. Fernando Vieira da Silva](https://www.kaggle.com/fernandojvdasilva/aplica-es-em-nlp-aula-02)
*   [101 NLP Exercises (using modern libraries)](https://www.machinelearningplus.com/nlp/nlp-exercises/)

Este notebook foi produzido por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500) e revisado por Carla Edila Silveira.
"""