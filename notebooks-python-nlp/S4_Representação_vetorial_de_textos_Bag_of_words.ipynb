{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XqPnuNhvNzF"
      },
      "source": [
        "># <b>Representação vetorial de textos – *Bag of words*</b>\n",
        ">Instituição: PUCPR  \n",
        ">Curso: Tecnologia em Inteligência Artifical Aplicada  \n",
        ">Disciplina: Processamento de Linguagem Natural  \n",
        ">Professor: Lucas Oliveira   \n",
        ">Estudante: Carla Edila Silveira  \n",
        "> Data: 14/04/2024\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## <b>Processamento de Linguagem Natural</b>\n",
        "Hoje trabalharemos com um assunto essencial ao PLN moderno, a representação vetorial de textos. Nesta aula você realizará atividades práticas relacionadas a técnica chamada  **Bag of words (BoW)**, visando:\n",
        "1. Entender o modelo Bag of Words\n",
        "2. Vetorizar textos utilizando a função `CountVectorizer` da biblioteca `scikit-learn`\n",
        "3. Compreender as limitações do modelo e como ele pode ser utilizado em tarefas de PLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjDaG25DpNC7"
      },
      "source": [
        "### **O que é Bag of Words?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUW_kwINp1_j"
      },
      "source": [
        "É uma técnica de PLN na qual transformamos textos em **vetores numéricos** para **extrair características do texto**. Tais características podem ser interpretadas por diversos algoritmos, incluindo (principalmente) os de *Machine Learning*.\n",
        "\n",
        "Apenas dois passos são necessários no algoritmo de BoW:\n",
        "1.   Determinar o vocabulário do(s) texto(s)\n",
        "2.   Realizar contagem do termos (frequência das palavras)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bJQdXUGS5G"
      },
      "source": [
        "Imagine um corpus com os seguintes documentos:\n",
        "\n",
        "*   *O menino correu*\n",
        "*   *O menino correu do cão*\n",
        "*   *O menino com o cão*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcwv9a9vI5Dn"
      },
      "source": [
        "#### 1) Determinar vocabulário\n",
        "Para determinar o vocabulário, basta definirmos uma lista com todas palavras contidas em nosso corpus.\n",
        "As palavras encontradas nos documentos acima são: `o`, `menino`, `correu`, `do`, `cão` e `com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAvE7uvaI7WS"
      },
      "source": [
        "#### 2) Contagem das palavras\n",
        "Nesta etapa devemos contar quantas vezes cada palavra do vocabulário aparece em cada documento/texto, e criamos um vetor com as quantidades computadas.\n",
        "\n",
        "![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1wDFX5RknqY8eBBaeurIqPK5KgwzjAKfd)\n",
        "\n",
        "Assim são gerados vetores individuais de cada documento:\n",
        "\n",
        "*   *O menino correu*: `[1, 1, 1, 0, 0, 0]`\n",
        "*   *O menino correu do cão*: `[1, 1, 1, 1, 1, 0]`\n",
        "*   *O menino com o cão*: `[2, 1, 0, 0, 1, 0]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3yLcnjzMWR7"
      },
      "source": [
        "\n",
        "\n",
        "> **SACO DE PALAVRAS?** A técnica tem esse nome porque se perde toda a informação contextual do texto, ou seja, perde-se onde cada palavra apareceu em cada documento, como se pegássemos todas palavras e colocássemos dentro de um saco!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnumO-E9OOPZ"
      },
      "source": [
        "> A matriz com as frequências das palavras também é chamada de **MATRIZ TERMO-DOCUMENTO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WyP5jmRSZdJ"
      },
      "source": [
        "**Mas, qual a ideia por trás do BoW?**\n",
        "\n",
        "O BoW segue a ideia de que **documentos semelhantes terão contagens de palavras semelhantes** entre si. Em outras palavras, quanto mais semelhantes forem as palavras em dois documentos, mais semelhantes poderão ser os documentos.\n",
        "Além disso, ao definir a matriz termo-documento, intui-se que **palavras com alta ocorrência em um documento, sejam importantes a ele**, ou seja, devem estar entre os temas centrais do texto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IWzM61JTA8A"
      },
      "source": [
        "**EXEMPLO**: Imagine os vetores a seguir:\n",
        "\n",
        "![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1dKga70Q9l3IRtW28TBqTcn8c_0kEQcrS)\n",
        "\n",
        "O vetor do documento 1 e 2 são similares (assim como seus textos). Já o vetor do documento 3 difere completamente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT1JQerSRnZl"
      },
      "source": [
        "### **Utilizando o scikit-learn para calcular o BoW ou matriz termo-documento**\n",
        "Apesar de ser uma técnica simples de se implementar, não há necessidade, pois ela já é implementada dentro da biblioteca `scikit-learn` sob o nome de `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3weUC_ph6is"
      },
      "source": [
        "# Exemplo de corpus\n",
        "corpus = [\n",
        "            \"o menino correu.\",\n",
        "            \"o menino correu do cão.\",\n",
        "            \"O Menino com o Cão.\"\n",
        "]\n",
        "\n",
        "# corpus = [\n",
        "#             \"gatos são bonitos\",\n",
        "#             \"gatos são lindos\",\n",
        "#             \"tomate é fruta\"\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSPlMg9rXer4"
      },
      "source": [
        "# Importa funcionalidade de BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c02VEUZLbGbh"
      },
      "source": [
        "# Cria instância de CountVectorizer\n",
        "vect = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QjwsyVxh2lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65f9159-464c-410d-90b2-94548426502e"
      },
      "source": [
        "# Transforma o corpus em vetores numéricos (BoW)\n",
        "X = vect.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 9 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8BO4Xdjv8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a02c4e-18ee-4ae3-e37f-60cbda4ca978"
      },
      "source": [
        "# Imprime a ordem de cada coluna\n",
        "# print(vect.get_feature_names()) - houve erro de execução\n",
        "print(list(vect.vocabulary_.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['menino', 'correu', 'do', 'cão', 'com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQP5vMlYirlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c6f469-52fe-46cd-9be9-0b267e6e6049"
      },
      "source": [
        "# Imprime vetores (BoW)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 0 1]\n",
            " [0 1 1 1 1]\n",
            " [1 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15lu78gJk01U"
      },
      "source": [
        "\n",
        "\n",
        "> **ATENÇÃO**: O CountVectorizer já transforma as palavras em `lowercase` por padrão, ignora pontuação e coloca as palavras em ordem alfabética nos vetores. Além disso ignora palavras que tenham frequência abaixo ou acima dos parâmetros `min_df` e `max_df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw4G0jHv4FEP"
      },
      "source": [
        "É possível também **vetorizar N-Grams** do corpus usando o CountVectorizer, sem necessidade de usar alguma função extra. Geralmente o fazemos para obter mais **contexto** do texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuntFPLT4O5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3f3f9f-bc25-4dca-eca6-ced006a5e4a6"
      },
      "source": [
        "# Cria instância de CountVectorizer\n",
        "# Apenas 2-grams serão gerados\n",
        "vect = CountVectorizer( ngram_range=(2,2) )\n",
        "\n",
        "# Transforma o corpus em vetores numéricos (BoW)\n",
        "X = vect.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 6 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdLCIU44kAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0880034-f967-4f74-e65e-76cdd0475da2"
      },
      "source": [
        "# Imprime a ordem de cada coluna\n",
        "# print(vect.get_feature_names()) - houve erro de execução\n",
        "print(list(vect.vocabulary_.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['menino correu', 'correu do', 'do cão', 'menino com', 'com cão']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEcBKZeZ4lXk"
      },
      "source": [
        "# Imprime vetores (BoW)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io8PJrSG1EKj"
      },
      "source": [
        "\n",
        "\n",
        "> **DICA**: Vale a pena olhar a [documentação](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do CountVectorizer, pois existem diversos parâmetros úteis que podemos utilizar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJXirFX4lb86"
      },
      "source": [
        "### **Limitações do BoW**\n",
        "Apesar das ideias de que **documentos semelhantes terão contagens de palavras semelhantes** entre si (ao aprender Classificação de Textos, você poderá ver isso mais detalhadamente) e que **uma palavra com alta frequência em um documento é considerada importante** funcionarem em vários casos, o modelo BoW tem algumas limitações, entre elas:\n",
        "\n",
        "*   **Peso igual a todas palavras**: o BoW dá um peso igual a todas palavras. Em nosso exemplo palavras como \"com\" e \"do\", tem o mesmo peso de \"cão\" e \"menino\". Isso não é bom, pois palavras mais comuns (artigos, preposições, etc) deveriam ter peso menor, por serem menos discriminantes.\n",
        "*   **Significado semântico**: a abordagem básica do BOW não considera o significado da palavra no documento. Ignora completamente o contexto em que é usado. A mesma palavra pode ser usada em vários locais com base no contexto ou nas palavras próximas (embora o uso de n-grams possa amenizar um pouco o problema do contexto).\n",
        "*   **Tamanho do vetor - maldição da dimensionalidade**: para um documento grande, o tamanho do vetor pode ser enorme, resultando em muito tempo de processamento e alto consumo de memória. Pode ser necessário ignorar as palavras com base na relevância do seu caso de uso.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY1sgfEUyGHI"
      },
      "source": [
        "### **Aplicações do BoW**\n",
        "É útil em qualquer tarefa em que a posição ou informação contextual do texto não é tão importante. Alguns exemplos são:\n",
        "\n",
        "*   Identificar o autor de um documento (**classificação de textos**)\n",
        "*   Agrupar documentos por tópicos (**clusterização**)\n",
        "*   Análise de sentimentos - identificar \"positividade\"/\"negatividade\" de um documento (**regressão**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J3mzwuLlK15"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA** - Análise de vetorização de corpus\n",
        "Nesta atividade, analisaremos um pequeno corpus fictício, realizaremos a vetorização do mesmo, discutiremos problemas no modelo e eventualmente tentaremos corrigí-los."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRdlq33PvSij"
      },
      "source": [
        "# Um corpus fictício com 8 documentos\n",
        "corpus = [\n",
        "          'Um caminhão está descendo rapidamente um morro.',#0\n",
        "          'O caminhão está rapidamente descendo o morro.',#1\n",
        "          'O menininho come bananas.',#2\n",
        "          'O menino comeu banana.',#3\n",
        "          'Pizza? Portuguesa!',#4\n",
        "          'Carro? Marea!',#5\n",
        "          'Qual Seu Nome?',#6\n",
        "          'qual seu nome?'#7\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iG73PGDVSEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8008e7-6cb2-4e68-e13f-dbedfff806a0"
      },
      "source": [
        "# Código da QUESTÃO 4\n",
        "# Inicializar o objeto CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# Ajustar e transformar o corpus\n",
        "vetorizado = vect.fit_transform(corpus)\n",
        "\n",
        "# Obtém as palavras presentes no vocabulário\n",
        "palavras = vect.get_feature_names_out()\n",
        "\n",
        "# Imprime as palavras presentes no vocabulário\n",
        "print(palavras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['banana' 'bananas' 'caminhão' 'carro' 'come' 'comeu' 'descendo' 'está'\n",
            " 'marea' 'menininho' 'menino' 'morro' 'nome' 'pizza' 'portuguesa' 'qual'\n",
            " 'rapidamente' 'seu' 'um']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgdGeTh2sHbo"
      },
      "source": [
        "#### 1) Mostre a quantidade de caracteres e tokens do corpus\n",
        "\n",
        "\n",
        "> **DICA**: É possível concatenar todas sentenças da lista em uma String apenas usando a função `join()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnnmzxDyzWaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442f23fc-2f43-4a9c-e662-21cce453930e"
      },
      "source": [
        "# Concatenar todas as sentenças em uma única string\n",
        "corpus_completo = ' '.join(corpus)\n",
        "\n",
        "# Contar o número de caracteres na string\n",
        "quantidade_caracteres = len(corpus_completo)\n",
        "\n",
        "# Contar o número de tokens (palavras) na string\n",
        "quantidade_tokens = len(corpus_completo.split())\n",
        "\n",
        "# Imprimir a quantidade de caracteres e tokens\n",
        "print(\"Quantidade de caracteres no corpus:\", quantidade_caracteres)\n",
        "print(\"Quantidade de tokens no corpus:\", quantidade_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de caracteres no corpus: 205\n",
            "Quantidade de tokens no corpus: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w8XckBQtJeu"
      },
      "source": [
        "#### 2) Mostre a quantidade de tokens únicos do corpus\n",
        "Ao saber a quantidade de tokens únicos, qual informação importante você pode afirmar a respeito de um possível BoW?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sml8Pd3FtJsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02165e85-5886-42e8-ac5a-23de76ee7a88"
      },
      "source": [
        "# Criar um conjunto de tokens únicos\n",
        "tokens_unicos = set(corpus_completo.split())\n",
        "\n",
        "# Contar o número de tokens únicos\n",
        "quantidade_tokens_unicos = len(tokens_unicos)\n",
        "\n",
        "# Imprimir a quantidade de tokens únicos\n",
        "print(\"Quantidade de tokens únicos no corpus:\", quantidade_tokens_unicos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de tokens únicos no corpus: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVAÇÃO: Saber a quantidade de tokens únicos no corpus é importante para entender a diversidade vocabular do corpus. Isso pode indicar o tamanho do vocabulário que será usado na construção de um modelo Bag of Words (BoW). Quanto maior o número de tokens únicos, mais rico e variado é o vocabulário do corpus. Isso também pode influenciar o tamanho e a complexidade do vetor de características resultante do modelo BoW."
      ],
      "metadata": {
        "id": "jD2jCGXccx7y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eNPe0R5rQBi"
      },
      "source": [
        "#### 3) Aplique o BoW no corpus e mostre as dimensões, palavras e os valores do vetor resultante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSSLzc1uB2P"
      },
      "source": [
        "# Inicializar o objeto CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "\n",
        "# Ajustar e transformar o corpus\n",
        "vetorizado = vect.fit_transform(corpus)\n",
        "\n",
        "# Obter as palavras presentes no vocabulário\n",
        "palavras = vect.get_feature_names_out()\n",
        "\n",
        "# Obter a matriz de características (vetor resultante)\n",
        "matriz_caracteristicas = vetorizado.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz5e9sCv1aN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d23fd8d-a6a8-4be1-ca8b-19c79f0f70c1"
      },
      "source": [
        "# Imprimir as dimensões da matriz de características\n",
        "print(\"Dimensões da matriz de características:\", matriz_caracteristicas.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensões da matriz de características: (8, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir as palavras presentes no vocabulário\n",
        "print(\"Palavras presentes no vocabulário:\")\n",
        "print(palavras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMjB8NKndNvT",
        "outputId": "754aca7b-2d80-4219-d075-3d6cad79d9f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras presentes no vocabulário:\n",
            "['banana' 'bananas' 'caminhão' 'carro' 'come' 'comeu' 'descendo' 'está'\n",
            " 'marea' 'menininho' 'menino' 'morro' 'nome' 'pizza' 'portuguesa' 'qual'\n",
            " 'rapidamente' 'seu' 'um']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIkNUXbS1aSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98977a5d-f5b1-4df3-a565-6df2577bca03"
      },
      "source": [
        "# Imprimir os valores do vetor resultante para cada documento\n",
        "print(\"Valores do vetor resultante:\")\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(\"Documento\", i, \":\", matriz_caracteristicas[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores do vetor resultante:\n",
            "Documento 0 : [0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 2]\n",
            "Documento 1 : [0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0]\n",
            "Documento 2 : [0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            "Documento 3 : [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            "Documento 4 : [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            "Documento 5 : [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Documento 6 : [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            "Documento 7 : [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilW30DVfUsEf"
      },
      "source": [
        "#### 4) Coloque o nome das colunas e valores do vetor em um `DataFrame` do `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihc-Obv5UsOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55335072-cd44-402a-e85b-6f0e06d43bda"
      },
      "source": [
        "import pandas as pd\n",
        "# Criar um DataFrame com os valores do vetor resultante\n",
        "df = pd.DataFrame(matriz_caracteristicas, columns=palavras)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   banana  bananas  caminhão  carro  come  comeu  descendo  está  marea  \\\n",
            "0       0        0         1      0     0      0         1     1      0   \n",
            "1       0        0         1      0     0      0         1     1      0   \n",
            "2       0        1         0      0     1      0         0     0      0   \n",
            "3       1        0         0      0     0      1         0     0      0   \n",
            "4       0        0         0      0     0      0         0     0      0   \n",
            "5       0        0         0      1     0      0         0     0      1   \n",
            "6       0        0         0      0     0      0         0     0      0   \n",
            "7       0        0         0      0     0      0         0     0      0   \n",
            "\n",
            "   menininho  menino  morro  nome  pizza  portuguesa  qual  rapidamente  seu  \\\n",
            "0          0       0      1     0      0           0     0            1    0   \n",
            "1          0       0      1     0      0           0     0            1    0   \n",
            "2          1       0      0     0      0           0     0            0    0   \n",
            "3          0       1      0     0      0           0     0            0    0   \n",
            "4          0       0      0     0      1           1     0            0    0   \n",
            "5          0       0      0     0      0           0     0            0    0   \n",
            "6          0       0      0     1      0           0     1            0    1   \n",
            "7          0       0      0     1      0           0     1            0    1   \n",
            "\n",
            "   um  \n",
            "0   2  \n",
            "1   0  \n",
            "2   0  \n",
            "3   0  \n",
            "4   0  \n",
            "5   0  \n",
            "6   0  \n",
            "7   0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t_lE9wr7Cgm"
      },
      "source": [
        "#### 5) Ao comparar os vetores resultantes e aplicar a função de Distância de Jaccard nos documentos 2 e 3 verificamos que eles são bem dissimilares, apesar de serem sentenças com valor semântico similar. **Como podemos tornar os vetores do BoW para estes documentos mais semelhantes?**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBSERVAÇÕES: Se os vetores resultantes do modelo Bag of Words (BoW) para os documentos 2 e 3 são bem dissimilares, apesar de serem sentenças com valor semântico similar, isso pode indicar que o BoW está falhando em capturar a semelhança semântica entre esses documentos. Para tornar os vetores do BoW mais semelhantes, podemos considerar algumas abordagens:\n",
        "\n",
        "1. **Pré-processamento de texto**: Antes de aplicar o BoW, podemos realizar pré-processamento de texto para reduzir as variações nos documentos, como remover stop words, aplicar stemming ou lematização e converter todas as palavras para minúsculas. Isso pode ajudar a aumentar a similaridade entre os documentos.\n",
        "\n",
        "2. **Ajustar parâmetros do CountVectorizer**: Podemos ajustar os parâmetros do CountVectorizer para considerar diferentes aspectos dos documentos, como o uso de n-gramas, a remoção de palavras raras ou muito comuns, e o uso de tokenização personalizada. Experimentar diferentes configurações pode levar a vetores mais semelhantes entre documentos semanticamente similares.\n",
        "\n",
        "3. **Uso de modelos de incorporação de palavras**: Em vez de usar o BoW, podemos considerar o uso de modelos de incorporação de palavras (word embeddings), como Word2Vec, GloVe ou FastText. Esses modelos capturam melhor a semântica das palavras e podem fornecer representações vetoriais mais ricas e semânticas dos documentos.\n",
        "\n",
        "4. **Combinação de características**: Podemos combinar os vetores do BoW com outras características, como contagens de bigramas, trigramas, TF-IDF ou características baseadas em semântica latente, para capturar melhor a semelhança semântica entre os documentos.\n",
        "\n",
        "5. **Uso de outras medidas de similaridade**: Além da Distância de Jaccard, podemos explorar outras medidas de similaridade, como a similaridade de cosseno, similaridade de Jaccard ponderada, similaridade de Jaccard baseada em TF-IDF, etc. Algumas dessas medidas podem ser mais adequadas para capturar a semelhança semântica entre os documentos.\n",
        "\n",
        "Ao experimentar essas abordagens e ajustar os parâmetros do modelo, podemos tentar melhorar a capacidade do BoW em capturar a semelhança semântica entre os documentos, tornando os vetores resultantes mais semelhantes para documentos semanticamente similares."
      ],
      "metadata": {
        "id": "mKh67NyseEKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa bibliotecas para pré-processamento\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYicRlalfJ_s",
        "outputId": "a9502a4f-9d3c-497d-dfc4-d6009072efef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar o objeto CountVectorizer com opções de pré-processamento\n",
        "stop_words = stopwords.words('portuguese')  # Obter lista de stop words em português\n",
        "stemmer = PorterStemmer()  # Inicializar o stemmer\n",
        "vect = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
        "\n",
        "# Realizar pré-processamento de texto para cada documento\n",
        "corpus_preprocessado = []\n",
        "for doc in corpus:\n",
        "    # Converter todas as palavras para minúsculas\n",
        "    doc = doc.lower()\n",
        "    # Tokenizar o documento\n",
        "    tokens = word_tokenize(doc)\n",
        "    # Remover stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Aplicar stemming\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # Juntar os tokens de volta em uma string\n",
        "    doc_preprocessado = ' '.join(tokens)\n",
        "    # Adicionar o documento pré-processado à lista\n",
        "    corpus_preprocessado.append(doc_preprocessado)"
      ],
      "metadata": {
        "id": "JWdNGKlAftoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar e transformar o corpus pré-processado\n",
        "vetorizado = vect.fit_transform(corpus_preprocessado)\n",
        "\n",
        "# Obter as palavras presentes no vocabulário\n",
        "palavras = vect.get_feature_names_out()\n",
        "\n",
        "# Obter a matriz de características (vetor resultante)\n",
        "matriz_caracteristicas = vetorizado.toarray()"
      ],
      "metadata": {
        "id": "OlAMv7-QfJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B3ku7XWOfJpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir as dimensões da matriz de características\n",
        "print(\"Dimensões da matriz de características:\", matriz_caracteristicas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-xGlQxFfJZy",
        "outputId": "67d3d27b-7030-4101-a593-f1ebe65896f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensões da matriz de características: (8, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHjh4wZH8Hy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef255f17-64f0-4796-d0ca-fe9df0c6ff57"
      },
      "source": [
        "# Imprimir as palavras presentes no vocabulário\n",
        "print(\"Palavras presentes no vocabulário:\")\n",
        "print(palavras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras presentes no vocabulário:\n",
            "['banana' 'caminhão' 'carro' 'come' 'comeu' 'descendo' 'marea' 'menininho'\n",
            " 'menino' 'morro' 'nome' 'pizza' 'portuguesa' 'rapidament']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKM0gHsn8Yn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf40a33-b39d-4e4e-8abb-da2374117b46"
      },
      "source": [
        "# Imprimir os valores do vetor resultante para cada documento\n",
        "print(\"Valores do vetor resultante:\")\n",
        "for i, doc in enumerate(corpus_preprocessado):\n",
        "    print(\"Documento\", i, \":\", matriz_caracteristicas[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores do vetor resultante:\n",
            "Documento 0 : [0 1 0 0 0 1 0 0 0 1 0 0 0 1]\n",
            "Documento 1 : [0 1 0 0 0 1 0 0 0 1 0 0 0 1]\n",
            "Documento 2 : [1 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
            "Documento 3 : [1 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
            "Documento 4 : [0 0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
            "Documento 5 : [0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
            "Documento 6 : [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            "Documento 7 : [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um DataFrame com os valores do vetor resultante\n",
        "df = pd.DataFrame(matriz_caracteristicas, columns=palavras)\n",
        "\n",
        "# Imprimir o DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gQTBxvdhxbB",
        "outputId": "8cd6c471-a595-4dc4-ee8d-3f573bb7df77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   banana  caminhão  carro  come  comeu  descendo  marea  menininho  menino  \\\n",
            "0       0         1      0     0      0         1      0          0       0   \n",
            "1       0         1      0     0      0         1      0          0       0   \n",
            "2       1         0      0     1      0         0      0          1       0   \n",
            "3       1         0      0     0      1         0      0          0       1   \n",
            "4       0         0      0     0      0         0      0          0       0   \n",
            "5       0         0      1     0      0         0      1          0       0   \n",
            "6       0         0      0     0      0         0      0          0       0   \n",
            "7       0         0      0     0      0         0      0          0       0   \n",
            "\n",
            "   morro  nome  pizza  portuguesa  rapidament  \n",
            "0      1     0      0           0           1  \n",
            "1      1     0      0           0           1  \n",
            "2      0     0      0           0           0  \n",
            "3      0     0      0           0           0  \n",
            "4      0     0      1           1           0  \n",
            "5      0     0      0           0           0  \n",
            "6      0     1      0           0           0  \n",
            "7      0     1      0           0           0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhKpej1E-CCl"
      },
      "source": [
        "Você pode alterar/incluir células nas questões anteriores para resolver essa questão. Depois re-execute as 3 células acima para checar os resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOIWXmB5fzF"
      },
      "source": [
        "#### 6) Gere um BoW com 2-grams e 3-grams\n",
        "Olhe a documentação do CountVectorizer para verificar como parametrizar para gerar os dois simultaneamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mouxcab_5e0r"
      },
      "source": [
        "# Inicializar o objeto CountVectorizer para extrair 2-grams e 3-grams\n",
        "vect = CountVectorizer(ngram_range=(2, 3))\n",
        "\n",
        "# Ajustar e transformar o corpus\n",
        "vetorizado = vect.fit_transform(corpus)\n",
        "\n",
        "# Obter as palavras presentes no vocabulário\n",
        "palavras = vect.get_feature_names_out()\n",
        "\n",
        "# Obter a matriz de características (vetor resultante)\n",
        "matriz_caracteristicas = vetorizado.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir as dimensões da matriz de características\n",
        "print(\"Dimensões da matriz de características:\", matriz_caracteristicas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ5yPi-3iIRo",
        "outputId": "6101ca4c-74ae-4356-9c2b-a08c961ebbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensões da matriz de características: (8, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir as palavras presentes no vocabulário\n",
        "print(\"Palavras presentes no vocabulário:\")\n",
        "print(palavras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWHsr7mSiQy8",
        "outputId": "90a0433d-ea37-476b-dab9-885c1a439043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras presentes no vocabulário:\n",
            "['caminhão está' 'caminhão está descendo' 'caminhão está rapidamente'\n",
            " 'carro marea' 'come bananas' 'comeu banana' 'descendo morro'\n",
            " 'descendo rapidamente' 'descendo rapidamente um' 'está descendo'\n",
            " 'está descendo rapidamente' 'está rapidamente'\n",
            " 'está rapidamente descendo' 'menininho come' 'menininho come bananas'\n",
            " 'menino comeu' 'menino comeu banana' 'pizza portuguesa' 'qual seu'\n",
            " 'qual seu nome' 'rapidamente descendo' 'rapidamente descendo morro'\n",
            " 'rapidamente um' 'rapidamente um morro' 'seu nome' 'um caminhão'\n",
            " 'um caminhão está' 'um morro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir os valores do vetor resultante para cada documento\n",
        "print(\"Valores do vetor resultante:\")\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(\"Documento\", i, \":\", matriz_caracteristicas[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxNnMSVXiUcv",
        "outputId": "6ca99488-4c5e-4ced-faef-161817f1deb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores do vetor resultante:\n",
            "Documento 0 : [1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1]\n",
            "Documento 1 : [1 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
            "Documento 2 : [0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Documento 3 : [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Documento 4 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Documento 5 : [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Documento 6 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0]\n",
            "Documento 7 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O gabarito para as atividades encontra-se [aqui](https://colab.research.google.com/drive/1-w8fIbyRnj9oPB03WNWBZgp-4eqcUhBr)."
      ],
      "metadata": {
        "id": "8s6mr25e8kvu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z04cjeunIiF8"
      },
      "source": [
        "## Referências e Material complementar\n",
        "\n",
        "*   [An Introduction to Bag-of-Words in NLP](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)\n",
        "*   [A Simple Explanation of the Bag-of-Words Model](https://victorzhou.com/blog/bag-of-words/)"
      ]
    }
  ]
}