{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XqPnuNhvNzF"
      },
      "source": [
        "# GABARITO\n",
        "# Representação vetorial de textos – *Bag of words*\n",
        "## Processamento de Linguagem Natural\n",
        "Hoje trabalharemos com um assunto essencial ao PLN moderno, a representação vetorial de textos. Nesta aula você realizará atividades práticas relacionadas a técnica chamada  **Bag of words (BoW)**, visando:\n",
        "1. Entender o modelo Bag of Words\n",
        "2. Vetorizar textos utilizando a função `CountVectorizer` da biblioteca `scikit-learn`\n",
        "3. Compreender as limitações do modelo e como ele pode ser utilizado em tarefas de PLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjDaG25DpNC7"
      },
      "source": [
        "### **O que é Bag of Words?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUW_kwINp1_j"
      },
      "source": [
        "É uma técnica de PLN na qual transformamos textos em **vetores numéricos** para **extrair características do texto**. Tais características podem ser interpretadas por diversos algoritmos, incluindo (principalmente) os de *Machine Learning*.\n",
        "\n",
        "Apenas dois passos são necessários no algoritmo de BoW:\n",
        "1.   Determinar o vocabulário do(s) texto(s)\n",
        "2.   Realizar contagem do termos (frequência das palavras)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5bJQdXUGS5G"
      },
      "source": [
        "Imagine um corpus com os seguintes documentos:\n",
        "\n",
        "*   *O menino correu*\n",
        "*   *O menino correu do cão*\n",
        "*   *O menino com o cão*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcwv9a9vI5Dn"
      },
      "source": [
        "#### 1) Determinar vocabulário\n",
        "Para determinar o vocabulário, basta definirmos uma lista com todas palavras contidas em nosso corpus.\n",
        "As palavras encontradas nos documentos acima são: `o`, `menino`, `correu`, `do`, `cão` e `com`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAvE7uvaI7WS"
      },
      "source": [
        "#### 2) Contagem das palavras\n",
        "Nesta etapa devemos contar quantas vezes cada palavra do vocabulário aparece em cada documento/texto, e criamos um vetor com as quantidades computadas.\n",
        "\n",
        "![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1wDFX5RknqY8eBBaeurIqPK5KgwzjAKfd)\n",
        "\n",
        "Assim são gerados vetores individuais de cada documento:\n",
        "\n",
        "*   *O menino correu*: `[1, 1, 1, 0, 0, 0]`\n",
        "*   *O menino correu do cão*: `[1, 1, 1, 1, 1, 0]`\n",
        "*   *O menino com o cão*: `[2, 1, 0, 0, 1, 0]`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3yLcnjzMWR7"
      },
      "source": [
        "\n",
        "\n",
        "> **SACO DE PALAVRAS?** A técnica tem esse nome pois perde-se toda informação contextual do texto, ou seja, onde cada palavra apareceu em cada documento, como se pegássemos todas palavras e colocássemos dentro de um saco!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnumO-E9OOPZ"
      },
      "source": [
        "> A matriz com as frequências das palavras também é chamada de **MATRIZ TERMO-DOCUMENTO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WyP5jmRSZdJ"
      },
      "source": [
        "**Mas, qual a ideia por trás do BoW?**\n",
        "\n",
        "O BoW segue a ideia de que **documentos semelhantes terão contagens de palavras semelhantes** entre si. Em outras palavras, quanto mais semelhantes forem as palavras em dois documentos, mais semelhantes poderão ser os documentos.\n",
        "Além disso, ao definir a matriz termo-documento, intui-se que **palavras com alta ocorrência em um documento, sejam importantes a ele**, ou seja, devem estar entre os temas centrais do texto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IWzM61JTA8A"
      },
      "source": [
        "**EXEMPLO**: Imagine os vetores a seguir:\n",
        "\n",
        "![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1dKga70Q9l3IRtW28TBqTcn8c_0kEQcrS)\n",
        "\n",
        "O vetor do documento 1 e 2 são similares (assim como seus textos). Já o vetor do documento 3 se difere completamente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT1JQerSRnZl"
      },
      "source": [
        "### **Utilizando o scikit-learn para calcular o BoW ou matriz termo-documento**\n",
        "Apesar de ser uma técnica simples de se implementar, não há necessidade, pois ela já é implementada dentro da biblioteca `scikit-learn` sob o nome de `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3weUC_ph6is"
      },
      "source": [
        "# Exemplo de corpus\n",
        "corpus = [\n",
        "            \"o menino correu.\",\n",
        "            \"o menino correu do cão.\",\n",
        "            \"O Menino com o Cão.\"\n",
        "]\n",
        "\n",
        "# corpus = [\n",
        "#             \"gatos são bonitos\",\n",
        "#             \"gatos são lindos\",\n",
        "#             \"tomate é fruta\"\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSPlMg9rXer4"
      },
      "source": [
        "# Importa funcionalidade de BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c02VEUZLbGbh"
      },
      "source": [
        "# Cria instância de CountVectorizer\n",
        "vect = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QjwsyVxh2lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5d5ead52-f5a2-4ae2-cacf-cc0bc84c4ce4"
      },
      "source": [
        "# Transforma o corpus em vetores numéricos (BoW)\n",
        "X = vect.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 9 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8BO4Xdjv8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d731038c-8c37-48a8-899f-7c5ef3070808"
      },
      "source": [
        "# Imprime a ordem de cada coluna\n",
        "print(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['com', 'correu', 'cão', 'do', 'menino']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQP5vMlYirlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d2098a3d-5973-4473-e433-80566dd9aa84"
      },
      "source": [
        "# Imprime vetores (BoW)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 0 1]\n",
            " [0 1 1 1 1]\n",
            " [1 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15lu78gJk01U"
      },
      "source": [
        "\n",
        "\n",
        "> **ATENÇÃO**: O CountVectorizer já transforma as palavras em `lowercase` por padrão, ignora pontuação e coloca as palavras em ordem alfabética nos vetores. Além disso ignora palavras que tenham frequência abaixo ou acima dos parâmetros `min_df` e `max_df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw4G0jHv4FEP"
      },
      "source": [
        "É possível também **vetorizar N-Grams** do corpus usando o CountVectorizer, sem necessidade de usar alguma função extra. Geralmente o fazemos para obter mais **contexto** do texto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuntFPLT4O5T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5509d810-1463-4685-b43f-2f134663598b"
      },
      "source": [
        "# Cria instância de CountVectorizer\n",
        "# Apenas 2-grams serão gerados\n",
        "vect = CountVectorizer( ngram_range=(2,2) )\n",
        "\n",
        "# Transforma o corpus em vetores numéricos (BoW)\n",
        "X = vect.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 6 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHdLCIU44kAC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e59b5346-6670-43d6-a72b-77be450691e5"
      },
      "source": [
        "# Imprime a ordem de cada coluna\n",
        "print(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['com cão', 'correu do', 'do cão', 'menino com', 'menino correu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEcBKZeZ4lXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bc96614d-c5e0-43e9-9d7c-3d82ac0e585f"
      },
      "source": [
        "# Imprime vetores (BoW)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 1]\n",
            " [0 1 1 0 1]\n",
            " [1 0 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io8PJrSG1EKj"
      },
      "source": [
        "\n",
        "\n",
        "> **DICA**: Vale a pena olhar a [documentação](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do CountVectorizer, pois existem diversos parâmetros úteis que podemos utilizar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJXirFX4lb86"
      },
      "source": [
        "### **Limitações do BoW**\n",
        "Apesar das ideias de que **documentos semelhantes terão contagens de palavras semelhantes** entre si (ao aprender Classificação de Textos você poderá ver isso mais detalhadamente) e que **uma palavra com alta frequência em um documento é considerada importante** funcionarem em vários casos, o modelo BoW tem algumas limitações, entre elas:\n",
        "\n",
        "*   **Peso igual a todas palavras**: o BoW dá um peso igual a todas palavras. Em nosso exemplo palavras como \"com\" e \"do\", tem o mesmo peso de \"cão\" e \"menino\". Isso não é bom pois palavras mais comuns (artigos, preposições, etc) deveriam ter peso menor, pois são menos discriminantes.\n",
        "*   **Significado semântico**: a abordagem básica do BOW não considera o significado da palavra no documento. Ignora completamente o contexto em que é usado. A mesma palavra pode ser usada em vários locais com base no contexto ou nas palavras próximas (embora o uso de n-grams possa amenizar um pouco o problema do contexto).\n",
        "*   **Tamanho do vetor - maldição da dimensionalidade**: para um documento grande, o tamanho do vetor pode ser enorme, resultando em muito tempo de processamento e alto consumo de memória. Pode ser necessário ignorar as palavras com base na relevância do seu caso de uso.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY1sgfEUyGHI"
      },
      "source": [
        "### **Aplicações do BoW**\n",
        "Ele é útil em qualquer tarefa em que a posição ou informação contextual do texto não é tão importante. Alguns exemplos são:\n",
        "\n",
        "*   Identificar o autor de um documento (**classificação de textos**)\n",
        "*   Agrupar documentos por tópicos (**clusterização**)\n",
        "*   Análise de sentimentos - identificar \"positividade\"/\"negatividade\" de um documento (**regressão**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J3mzwuLlK15"
      },
      "source": [
        "### **ATIVIDADE PRÁTICA** - Análise de vetorização de corpus\n",
        "Nesta atividade iremos analisar um pequeno corpus fictício, realizar a vetorização do mesmo, discutir problemas no modelo e eventualmente tentar corrigí-los."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRdlq33PvSij"
      },
      "source": [
        "# Um corpus fictício com 8 documentos\n",
        "corpus = [\n",
        "          'Um caminhão está descendo rapidamente um morro.',#0\n",
        "          'O caminhão está rapidamente descendo o morro.',#1\n",
        "          'O menininho come bananas.',#2\n",
        "          'O menino comeu banana.',#3\n",
        "          'Pizza? Portuguesa!',#4\n",
        "          'Carro? Marea!',#5\n",
        "          'Qual Seu Nome?',#6\n",
        "          'qual seu nome?'#7\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iG73PGDVSEN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a5784572-aeeb-4866-d333-b33f16a2e44b"
      },
      "source": [
        "# Código da QUESTÃO 4\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "\n",
        "new_corpus=[' '.join([stemmer.stem(word) for word in tokenize.word_tokenize(text, language='portuguese')]) for text in corpus]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4derm7RXMRu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bbec1a0d-45fc-4e5a-c13e-9bfe5c496e80"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Um caminhão está descendo rapidamente um morro.',\n",
              " 'O caminhão está rapidamente descendo o morro.',\n",
              " 'O menininho come bananas.',\n",
              " 'O menino comeu banana.',\n",
              " 'Pizza? Portuguesa!',\n",
              " 'Carro? Marea!',\n",
              " 'Qual Seu Nome?',\n",
              " 'qual seu nome?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6pB7pB-XOUh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1e8bbfff-8420-4762-ae69-85ebc7c21f31"
      },
      "source": [
        "new_corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['um caminh est desc rapid um morr .',\n",
              " 'o caminh est rapid desc o morr .',\n",
              " 'o menin com banan .',\n",
              " 'o menin com banan .',\n",
              " 'pizz ? portugu !',\n",
              " 'carr ? mare !',\n",
              " 'qual seu nom ?',\n",
              " 'qual seu nom ?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUmPaymhXmIo"
      },
      "source": [
        "corpus = new_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgdGeTh2sHbo"
      },
      "source": [
        "#### 1) Mostre a quantidade de caracteres e tokens do corpus\n",
        "\n",
        "\n",
        "> **DICA**: É possível concatenar todas sentenças da lista em uma String apenas usando a função `join()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnnmzxDyzWaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1a38c2ac-c671-41c8-dba9-c29cf453a502"
      },
      "source": [
        "todotexto = ' '.join(corpus)\n",
        "todotexto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'um caminh est desc rapid um morr . o caminh est rapid desc o morr . o menin com banan . o menin com banan . pizz ? portugu ! carr ? mare ! qual seu nom ? qual seu nom ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwrAY-bzsHu9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11b31eed-cce8-49e7-8da8-e31f7e2c14f8"
      },
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = tokenize.word_tokenize(todotexto, language='portuguese')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUX3oUHMst_b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e1f9883-03c6-4855-cde8-68cc4074fefd"
      },
      "source": [
        "len(todotexto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMFAI3MPsvMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2f0c39d-8b9e-44c9-fca3-48082fee3d83"
      },
      "source": [
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w8XckBQtJeu"
      },
      "source": [
        "#### 2) Mostre a quantidade de tokens únicos do corpus\n",
        "Ao saber a quantidade de tokens únicos, qual informação importante você pode afirmar a respeito de um possível BoW?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sml8Pd3FtJsp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4de9f569-e68a-4cd0-87d9-f5c41edc340d"
      },
      "source": [
        "len(set(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty7Lblim2DMC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "0a755993-0a24-4e83-f3cb-11ad6005ce35"
      },
      "source": [
        "# Após gerar o vetor, podemos mostrar para os alunos pq houve diferença na quantidade de tokens únicos para a dimensão dos vetores\n",
        "set(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!',\n",
              " '.',\n",
              " '?',\n",
              " 'banan',\n",
              " 'caminh',\n",
              " 'carr',\n",
              " 'com',\n",
              " 'desc',\n",
              " 'est',\n",
              " 'mare',\n",
              " 'menin',\n",
              " 'morr',\n",
              " 'nom',\n",
              " 'o',\n",
              " 'pizz',\n",
              " 'portugu',\n",
              " 'qual',\n",
              " 'rapid',\n",
              " 'seu',\n",
              " 'um'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eNPe0R5rQBi"
      },
      "source": [
        "#### 3) Aplique o BoW no corpus e mostre as dimensões, palavras e os valores do vetor resultante"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNSSLzc1uB2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7c5747a3-44b3-465e-9a2a-9c9e2da74827"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "X = vect.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<8x16 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 27 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz5e9sCv1aN9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b3eb018c-dac1-44ef-d315-a032854328b7"
      },
      "source": [
        "print(vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['banan', 'caminh', 'carr', 'com', 'desc', 'est', 'mare', 'menin', 'morr', 'nom', 'pizz', 'portugu', 'qual', 'rapid', 'seu', 'um']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIkNUXbS1aSI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7d50f267-13eb-4982-bff8-a2f21af9740b"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 2]\n",
            " [0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilW30DVfUsEf"
      },
      "source": [
        "#### 4) Coloque o nome das colunas e valores do vetor em um `DataFrame` do `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihc-Obv5UsOT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a741af18-2cf4-42fb-8eff-17ed7ca9c027"
      },
      "source": [
        "import pandas as pd\n",
        "results = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>banan</th>\n",
              "      <th>caminh</th>\n",
              "      <th>carr</th>\n",
              "      <th>com</th>\n",
              "      <th>desc</th>\n",
              "      <th>est</th>\n",
              "      <th>mare</th>\n",
              "      <th>menin</th>\n",
              "      <th>morr</th>\n",
              "      <th>nom</th>\n",
              "      <th>pizz</th>\n",
              "      <th>portugu</th>\n",
              "      <th>qual</th>\n",
              "      <th>rapid</th>\n",
              "      <th>seu</th>\n",
              "      <th>um</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   banan  caminh  carr  com  desc  est  ...  pizz  portugu  qual  rapid  seu  um\n",
              "0      0       1     0    0     1    1  ...     0        0     0      1    0   2\n",
              "1      0       1     0    0     1    1  ...     0        0     0      1    0   0\n",
              "2      1       0     0    1     0    0  ...     0        0     0      0    0   0\n",
              "3      1       0     0    1     0    0  ...     0        0     0      0    0   0\n",
              "4      0       0     0    0     0    0  ...     1        1     0      0    0   0\n",
              "5      0       0     1    0     0    0  ...     0        0     0      0    0   0\n",
              "6      0       0     0    0     0    0  ...     0        0     1      0    1   0\n",
              "7      0       0     0    0     0    0  ...     0        0     1      0    1   0\n",
              "\n",
              "[8 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t_lE9wr7Cgm"
      },
      "source": [
        "#### 5) Ao comparar os vetores resultantes e aplicar a função de Distância de Jaccard nos documentos 2 e 3 verificamos que eles são bem dissimilares, apesar de serem sentenças com valor semântico similar. **Como podemos tornar os vetores do BoW para estes documentos mais semelhantes?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDMDuhe47Ct9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3702720-01a2-4d40-8226-23c14873d710"
      },
      "source": [
        "# Imprime vetor do documento 2\n",
        "print(X[2].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHjh4wZH8Hy2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "054f6024-e63b-4963-87d9-05fe44806f49"
      },
      "source": [
        "# Imprime vetor do documento 3\n",
        "print(X[3].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKM0gHsn8Yn-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50e497bd-1042-4e82-e574-8c813fa9983f"
      },
      "source": [
        "d2_set = set(nltk.word_tokenize(corpus[2], language='portuguese'))\n",
        "d3_set = set(nltk.word_tokenize(corpus[3], language='portuguese'))\n",
        "\n",
        "#Calcula Jaccard\n",
        "nltk.jaccard_distance(d2_set, d3_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhKpej1E-CCl"
      },
      "source": [
        "Você pode alterar/incluir células nas questões anteriores para resolver essa questão. Depois re-execute as 3 células acima para checar os resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaOIWXmB5fzF"
      },
      "source": [
        "#### 6) Gere um BoW com 2-grams e 3-grams\n",
        "Olhe a documentação do CountVectorizer para verificar como parametrizar para gerar os dois simultaneamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mouxcab_5e0r"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z04cjeunIiF8"
      },
      "source": [
        "## Referências e Material complementar\n",
        "\n",
        "*   [An Introduction to Bag-of-Words in NLP](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)\n",
        "*   [A Simple Explanation of the Bag-of-Words Model](https://victorzhou.com/blog/bag-of-words/)"
      ]
    }
  ]
}