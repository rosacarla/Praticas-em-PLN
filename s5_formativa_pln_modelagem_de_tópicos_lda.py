# -*- coding: utf-8 -*-
"""S5-formativa-pln-Modelagem de tópicos - LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13uR9G9dEnEDUgHYIBfzdgniN6jTJxwjJ

# Modelagem de tópicos – *Latent Dirichlet Allocation (LDA)*
## Processamento de Linguagem Natural
Nesta aula trabalharemos com uma tarefa de PLN muito popular, a Modelagem de Tópicos. O objetivo é que ao final desta aula você:
1. Entenda o que é a Modelagem de tópicos
2. O que a diferencia de outras técnicas?
3. Compreenda e aplique o algoritmo *Latent Dirichlet Allocation (**LDA**)*

### **O que é Modelagem de tópicos?**

É um modelo estatístico para descobrir "tópicos" que ocorrem em uma coleção de documentos.

Uma ferramenta de modelagem de tópicos tenta "injetar" significado semântico ao **procurar padrões de uso de palavras em meio ao texto**. Para o computador, um tópico é uma lista de palavras que ocorrem em jeitos estatísticamente significativos.

Estes algoritmos não entendem o significado das palavras, eles apenas assumem que qualquer pedaço de texto é composto ao selecionarmos palavras de cestos de palavras, onde **cada cesto corresponde a um tópico**. Neste caso então, é possível decompor o texto em prováveis cestos de onde as palarvas vieram.


![LDA - Topic Modeling - Probabilistic
Topic Models - Blei, 2012](https://docs.google.com/uc?export=download&id=1wpyDoKwzJ8ynMmB_eM3ryG4mfjeNy0-X)

#### O TF-IDF também consegue identificar *keywords*, então qual a diferença entre as duas técnicas?

Apesar das duas técnicas conseguirem isolar e rankear termos importantes em um documento, as duas são muito diferentes entre si.

O **TF-IDF** por exemplo, é mais apropriado quando você quer uma visão ampla do seu corpus ainda na fase de **análise exploratória dos dados**, pois o algoritmo é transparente em suas pontuações e pode ser facilmente replicável.

É importante lembrar, que **os tópicos gerados na modelagem de tópicos nem sempre serão coerentes**. Mesmo assim, a modelagem de tópicos também pode ser útil para explorar os dados, pois conseguem sugerir categorias amplas ou clusters de textos dentro da coleção.

Os modelos de tópicos são especialmente atraentes porque os documentos recebem pontuações de como eles se encaixam em cada tópico e porque os tópicos são representados como listas de termos co-ocorrentes, o que **fornece um forte senso de como os termos se relacionam com os agrupamentos**. No entanto, o modelo probabilístico por trás dos modelos de tópicos é sofisticado e é fácil distorcer seus resultados se você não entender o que está fazendo. Já a matemática por trás do tf-idf é simples o suficiente para ser representada em uma planilha.

#### Mas quando usar Modelagem de tópicos então?

Antes de utilizar, você deve entender a real utilidade desta técnica. Por exemplo, se você terá acesso a uma pequena coleção de documentos, ou até mesmo um único documento, basta utilizar algo que conte a frequência das palavras (BoW, TF-IDF) que será o suficiente.

Já se você tem acesso a **centenas de documentos**, e deseja **compreender** melhor do que se tratam tais dados sem ter de realizar a leitura destes documentos, então a **Modelagem de tópicos provavelmente seja uma boa abordagem**.

##### Exemplo

Um exemplo seria a análise de discursos de um político, onde o algoritmo poderia retornar uma lista de tópicos e palavras-chave que compõe estes tópicos.

- *emprego empregos perda desemprego*
- *economia mercado banco moeda*
- *covid pandemia gripe quarentena*
- *pt lula dilma esquerda*

Ao verificar estas palavras-chave conseguimos verificar que o político em questão está preocupado com empregos, economia, a pandemia e com a esquerda.

### **Latent Dirichlet Allocation (LDA)**

Latent Dirichlet Allocation (LDA) é o método mais popular para
modelagem de tópicos. Baseia-se na distribuição probabilística do matemático Dirichlet. O LDA segue as seguintes intuiçoes:
- Documentos com tópicos similares usam grupos similares de palavras
- Tópicos podem então ser achados ao buscarmos grupos de palavras que acontecem juntas frequentemente em documentos do corpus

#### Dados
Nós usaremos os dados de notícias disponíveis na própria biblioteca sklearn. As notícias são em inglês, mas o código funciona exatamante da mesma maneira caso usemos em pt-br.
"""

from sklearn.datasets import fetch_20newsgroups

dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
documents = dataset.data
# Imprime o tamanho do dataset
len(documents)

# Imprime os grupos de notícias
dataset.target_names

"""#### Pré-processamento"""

import pandas as pd

noticiasDf = pd.DataFrame({'documento':documents})

# Remove tudo que não seja letra
noticiasDf['documento_limpo'] = noticiasDf['documento'].str.replace("[^a-zA-Z#]", " ")

# Remove palavras curtas
noticiasDf['documento_limpo'] = noticiasDf['documento_limpo'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))

# Transforma tudo em minúsculas
noticiasDf['documento_limpo'] = noticiasDf['documento_limpo'].apply(lambda x: x.lower())

"""

> **PERGUNTA**: Por que você acha que estas ações podem ser úteis?
Ajuda na redução da dimensionalidade, diminiu o tamanho doi vocabulário, pode ser ruim por retirar informações relevantes, como a marca HP em texto do domínio de TI.

"""

noticiasDf
# Saida de texto comum e texto limpo

"""Vamos agora utilizar o **CountVectorizer** para construir nossa **matriz termo-documento**. Já aproveitaremos alguns recursos da biblioteca para realizar mais etapas de pré-processamento."""

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')

mtd = cv.fit_transform(noticiasDf['documento_limpo'])

# docs x words
mtd
# Tamanho da matriz com nro de documentos e palavras diferentes

"""> **PERGUNTA**: Para que servem os parâmetros passados para a função CountVectorizer? Servem para pré-processar e reduzir o tamanho da matriz.

Verifique a documentação e re-execute o código sem passar os parâmetros.

#### **LDA**
O LDA já é implementado pelo sklearn
"""

from sklearn.decomposition import LatentDirichletAllocation
# Define quantidade de tópicos/clusters, escolheu-se 20 pra verificar se algoritmo consegue encontrar mesmas categorias pré-estabelecidas no dataset
# Definir o random_state faz com que a aleatoriedade na seleção das palavras seja a mesma, toda vez que este código for executado
LDA = LatentDirichletAllocation(n_components=20,random_state=42) # quantos topicos a encontrar = 20 temáticas diferentes
# Pode demorar se há muitos dados
LDA.fit(mtd)

"""Como obter o **vocabulário**?"""

cv.get_feature_names_out()

len(cv.get_feature_names_out())

# Você pode obter qualquer palavra do vocabulário
cv.get_feature_names_out()[20078] #acessa pela posição do indice

"""Como obter os **tópicos**?"""

# Quantidade de tópicos
len(LDA.components_)

# Cada tópico é um array com as probabilidades de cada palavra
LDA.components_

# tópicos x palavras
LDA.components_.shape

"""Como obter as **palavras com maior probabilidade** para cada tópico?"""

# Obtém primeiro tópico - ainda não sabemos do que se trata
primeiro_topico = LDA.components_[0]

# Obtemos os índices ordenados do menor pro maior
primeiro_topico.argsort()

"""

> **ENTENDA MELHOR**: Veja no exemplo a seguir o que o comando argsort() faz

"""

import numpy as np
arr = np.array([5, 50, 1, 10, 15])
# Ordena a posição dos elementos do array
arr.argsort()

"""Voltando para nossos textos...
Agora sabemos a localização palavras com maior valor de probabilidade!
"""

# Obtém os últimos dez valores - que como ordenamos, serão os que tem maior valor de probabilidade
top_10 = primeiro_topico.argsort()[-10:]

top_10

"""Agora que temos os índices, basta imprimir os valores"""

for i in top_10:
  print(cv.get_feature_names_out()[i])

"""> **PERGUNTAS**: As palavras fazem sentido juntas? A qual tipo de tópico você acha que os documentos deste cluster pertencem?

As vezes os tópicos podem ter uma grande intersecção, ou pouco representativos. Não há um número mágico, você deve avaliar!



"""

# Imprime todos as top palavras de cada tópico
for i, topic in enumerate(LDA.components_):
  print("\n\n=== TOP 15 palavras - Tópico ", i)
  print([cv.get_feature_names_out()[i] for i in topic.argsort()[-15:]])

"""Como definir a **probabilidade de um DOCUMENTO pertencer a um tópico**?

Ou seja, como atribuir os tópicos descobertos para os documentos do corpus?
"""

# Primeiro, lembre-se que temos nossa matriz termo-documento
mtd

# Também temos o DataFrame com os textos originais
noticiasDf

"""Vamos criar uma nova coluna neste DataFrame com o número do tópico correspondente"""

# Obtém as probabilidades de cada tópico por texto
topicos_prob = LDA.transform(mtd)

# Textos x Tópicos
topicos_prob.shape

# Probabilidades de cada tópico para o PRIMEIRO DOCUMENTO do corpus
topicos_prob[0]

"""Abra o documento em questão e verifique se o texto tem realmente relação com as palavras associadas ao tópico. Faça isso para vários documentos."""

noticiasDf['documento'][0]

# Obtém a posição da maior probabilidade para cada documento e coloca na nova coluna
noticiasDf['topico'] = topicos_prob.argmax(axis=1)
noticiasDf

"""### ATIVIDADE PRÁTICA
Agora é com você, faça algumas alterações nas chamadas para analisar os resultados.

*   Tente reduzir o número de componentes e encontrar uma quantidade que faça mais sentido para você

"""

# Define quantidade de tópicos/clusters, escolheu-se 20 pra verificar se algoritmo consegue encontrar mesmas categorias pré-estabelecidas no dataset
# Definir o random_state faz com que a aleatoriedade na seleção das palavras seja a mesma, toda vez que este código for executado
LDA = LatentDirichletAllocation(n_components=12,random_state=42) # quantos topicos a encontrar = 20 temáticas diferentes
# Pode demorar se há muitos dados
LDA.fit(mtd)

# Quantidade de tópicos
len(LDA.components_)

# Cada tópico é um array com as probabilidades de cada palavra
LDA.components_

# tópicos x palavras
LDA.components_.shape

# Obtém primeiro tópico - ainda não sabemos do que se trata
primeiro_topico = LDA.components_[0]

# Obtemos os índices ordenados do menor pro maior
primeiro_topico.argsort()

# Obtém os últimos dez valores - que como ordenamos, serão os que tem maior valor de probabilidade
top_10 = primeiro_topico.argsort()[-10:]

top_10

# Imprime os valores do top 10
for i in top_10:
  print(cv.get_feature_names_out()[i])

# Imprime todos as top palavras de cada tópico
for i, topic in enumerate(LDA.components_):
  print("\n\n=== TOP 15 palavras - Tópico ", i)
  print([cv.get_feature_names_out()[i] for i in topic.argsort()[-15:]])

# Obtém as probabilidades de cada tópico por texto
topicos_prob = LDA.transform(mtd)

# Textos x Tópicos
topicos_prob.shape

# Probabilidades de cada tópico para o PRIMEIRO DOCUMENTO do corpus
topicos_prob[0]

# Abre o documento
noticiasDf['documento'][0]

# Probabilidades de cada tópico para o PRIMEIRO DOCUMENTO do corpus
topicos_prob[8]

# Abre o documento
noticiasDf['documento'][8]

# Obtém a posição da maior probabilidade para cada documento e coloca na nova coluna
noticiasDf['topico'] = topicos_prob.argmax(axis=1)
noticiasDf

"""## Referências e Material complementar

* [LDA - Artigo original](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)  
* [Modelagem de tópicos - Prof. Walmes Zeviani](http://www.leg.ufpr.br/~walmes/ensino/mintex/slides/08-topicos.pdf)
* [Tutorial de LSA - Latent Semantic Analysis](https://pessoalex.wordpress.com/2019/04/01/uma-introducao-a-modelagem-de-topicos-utilizando-analise-semantica-latente-em-python/)
* [LDA in Python – How to grid search best topic models?](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/)
* [Topic modeling visualization – How to present the results of LDA models?](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)

Este notebook foi produzido por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500).
"""