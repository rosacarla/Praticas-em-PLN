{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rosacarla/Praticas-em-PLN/blob/main/%5BLLMs%5D_Respondendo_perguntas_sobre_arquivos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Como criar um chatGPT para seus próprios arquivos?\n",
        "Nesta atividade você verá como utilizar o **LangChain** para integrar uma **busca semântica** e **LLMs** para criação de um chatbot para responder perguntas referentes à uma coleção de arquivos. Neste exemplo utilizaremos algumas obras de Machado de Assis que convenientemente já estão disponíveis na biblioteca NLTK.\n",
        "\n",
        "Recomendo acesso à [documentação](https://python.langchain.com/) para conhecer todos os recursos da ferramenta."
      ],
      "metadata": {
        "id": "0rxOfDHBLPo1"
      },
      "id": "0rxOfDHBLPo1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Instalar as dependências\n",
        "Esta ação pode demorar alguns minutos."
      ],
      "metadata": {
        "id": "jNxcp7wgMqRL"
      },
      "id": "jNxcp7wgMqRL"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install python-magic\n",
        "!pip install unstructured\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "4gwSHAF-KW81"
      },
      "id": "4gwSHAF-KW81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala biblipteca Langchain Community\n",
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "cteRHfapyQdQ"
      },
      "id": "cteRHfapyQdQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Importar as bibliotecas necessárias"
      ],
      "metadata": {
        "id": "JjEDtCprMwjs"
      },
      "id": "JjEDtCprMwjs"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad66c9aa",
      "metadata": {
        "id": "ad66c9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6639d2f-d4b1-45d8-962d-3ae54d04b0b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package machado to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "import magic\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import machado\n",
        "nltk.download('machado')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Definir sua chave de acesso à API da OpenAI\n",
        "\n",
        "Como vamos acessar a API da OpenAI para geração dos embeddings e acesso ao modelo conversacional, é necessária a utilização de uma chave, fornecida nas [configurações de sua conta](https://platform.openai.com/account/api-keys). A execução deste exemplo exige cadastro de informações financeiras junto a OpenAI, pois o acesso a API tem custos. Porém, na criação da conta geralmente lhe são dados [$5 de créditos para testar](https://openai.com/pricing) (quantia mais que suficiente para execução deste código).\n",
        "\n",
        "![2023-07-13 14_43_55-Window.png](data:image/png;base64,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)\n",
        "\n",
        "> **DICA**: *Ao criar sua chave, guarde-a em um local seguro, pois a plataforma não o deixará visualizá-la novamente após a criação.*\n",
        "\n",
        "O LangChain também permite que utilizemos modelos próprios para geração dos embeddings e QA. Aqui utilizamos a API da OpenAI por mera conveniência.\n",
        "\n",
        "> **DICA**: *Geralmente é uma boa prática construir sua primeira versão da aplicação conversacional baseada na API da OpenAI, seja no GPT 3.5 ou GPT 4, para depois desenvolver/ajustar seus próprios modelos, caso necessário.*\n",
        "\n"
      ],
      "metadata": {
        "id": "yg9Jg0IRM4YC"
      },
      "id": "yg9Jg0IRM4YC"
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"COLOQUE SUA API KEY AQUI\")"
      ],
      "metadata": {
        "id": "w3eKUO0SNDHM"
      },
      "id": "w3eKUO0SNDHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Carregar documentos e separá-los em textos menores\n",
        "Precisamos que os documentos sejam separados em textos menores para que primeiro busquemos os trechos de texto que contém a resposta de nossa pergunta, para depois passá-lo ao modelo conversacional como input para ele encontrar a informação necessária e dar a resposta.\n",
        "\n",
        "> **IMPORTANTE**: *Existem certas limitações de tamanho de entrada nos LLMs, além disso, quanto mais texto passamos para API, maior o custo. Por este motivo vamos limitar este exemplo em algumas poucas obras de Machado de Assis.*\n",
        "\n",
        "> **OBS:**: *Você pode verificar os títulos das obras de Machado de Assis no NLTK através de* ```machado.readme()```.\n",
        "\n"
      ],
      "metadata": {
        "id": "PBy3OzKaO2-c"
      },
      "id": "PBy3OzKaO2-c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define um diretório para armazenar as obras\n",
        "target_folder = \"machado\"\n",
        "if (not os.path.exists(target_folder)):\n",
        "  os.mkdir(target_folder)\n",
        "\n",
        "# Define quais obras desejamos utilizar - aqui deixamos \"Memórias Póstumas de Brás Cubas\", \"Quincas Borba\" e \"Dom Casmurro\"\n",
        "file_paths  = ['romance/marm05.txt', 'romance/marm07.txt', 'romance/marm08.txt']\n",
        "# Caso queira todas obras, pode descomentar a linha abaixo, lembrando que os processamentos podem ficar mais lentos e os custos também aumentam caso utilize a API da OpenAI\n",
        "#file_paths = machado.fileids()\n",
        "\n",
        "# Percorre corpora de livros de Machado de Assis do NLTK\n",
        "for file_path in file_paths:\n",
        "\n",
        "  # Obtém conteúdo do livro\n",
        "  content = machado.raw(file_path)\n",
        "\n",
        "  # Define nome do arquivo de destino\n",
        "  copy_file_path = target_folder + \"/\" + file_path.split(\"/\")[-1]\n",
        "\n",
        "  # Escreve o conteúdo do livro em um arquivo dentro da pasta \"machado\"\n",
        "  with open(copy_file_path, 'w') as copy_file:\n",
        "    copy_file.write(content)"
      ],
      "metadata": {
        "id": "k_LuiOfWpaCN"
      },
      "id": "k_LuiOfWpaCN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a28a08",
      "metadata": {
        "id": "e8a28a08"
      },
      "outputs": [],
      "source": [
        "# Define o diretório onde estão dos arquivos, e define que deve abrir apenas arquivos com a extensão \".txt\"\n",
        "loader = DirectoryLoader('machado/', glob='**/*.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9740d9",
      "metadata": {
        "id": "6a9740d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3de4ff-40ba-4d53-ed5e-e94424010faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Transforma os arquivos em documentos do LangChain\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3153f864",
      "metadata": {
        "id": "3153f864"
      },
      "outputs": [],
      "source": [
        "# Define o tamanho dos textos menores que criaremos a partir dos documentos (1000 caracteres cada)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a792c6fb",
      "metadata": {
        "id": "a792c6fb"
      },
      "outputs": [],
      "source": [
        "# Efetua a separação dos documentos em textos menores\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Transformar os textos em embeddings (vetores numéricos)\n",
        "Neste ponto pegamos todos os textos que temos, transformamos eles em embeddings e armazenamos para que possamos efetuar buscas nos vetores numéricos depois."
      ],
      "metadata": {
        "id": "gJAeJIQQUTuJ"
      },
      "id": "gJAeJIQQUTuJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cad0de",
      "metadata": {
        "id": "d2cad0de"
      },
      "outputs": [],
      "source": [
        "# Obtém modelo de criação de embeddings da OpenAI (aqui poderíamos utilizar qualquer outro modelo - Ex.: https://python.langchain.com/docs/modules/data_connection/text_embedding/integrations/huggingfacehub)\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734ed265",
      "metadata": {
        "id": "734ed265"
      },
      "outputs": [],
      "source": [
        "# Armazena embeddings para uma busca posterior (há vários \"vector stores\" possíveis de se utilizar, aqui vamos de FAISS - Ex.: https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
        "docsearch = FAISS.from_documents(texts, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Carregar o LLM conversacional\n",
        "Aqui definimos qual modelo conversacional queremos utilizar. Vamos carregar algum modelo disponível na própria API da OpenAI, mas você poderia selecionar  um modelo próprio ou algum disponibilizado no [HuggingFace](https://python.langchain.com/docs/modules/model_io/models/llms/integrations/huggingface_hub), por exemplo."
      ],
      "metadata": {
        "id": "AY4H1n_7a4O6"
      },
      "id": "AY4H1n_7a4O6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66826924",
      "metadata": {
        "id": "66826924"
      },
      "outputs": [],
      "source": [
        "# Carrega LLM da OpenAI\n",
        "llm = OpenAI(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) Definir o recuperador de informação (Retriever)\n",
        "Neste momento definimos o LLM conversacional que instanciamos anteriormente, como o recuperador irá enviar a pergunta ao modelo, e onde estão os embeddings para busca dos textos relevantes.\n",
        "\n",
        "No caso do ```chain_type='stuff'```, é como se o LangChain enviasse o prompt a seguir para o modelo conversacional, substituindo o ```{context}``` pelos textos encontrados na busca, e o ```{question}``` pela pergunta feita pelo usuário.\n",
        "\n",
        "```\n",
        "Use as seguintes partes do contexto para responder à pergunta dos usuários.\n",
        "Se você não souber a resposta, apenas diga que não sabe, não tente inventar uma resposta.\n",
        "{context}\n",
        "{question}\n",
        "```"
      ],
      "metadata": {
        "id": "Jab6lnfodznB"
      },
      "id": "Jab6lnfodznB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817a0ece",
      "metadata": {
        "id": "817a0ece"
      },
      "outputs": [],
      "source": [
        "# Cria o retriever\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Enviar pergunta ao chatbot\n",
        "Para enviar perguntas ao chatbot, basta enviá-la ao Retriever criado."
      ],
      "metadata": {
        "id": "g-b-W06HhgL6"
      },
      "id": "g-b-W06HhgL6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b81063",
      "metadata": {
        "id": "20b81063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cff64243-d6b6-43db-c0d9-554b5e75375a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Bentinho ganhou a alcunha de Dom Casmurro depois que ele adormeceu durante a recitação de versos por um rapaz no trem e o rapaz ficou amuado.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Envia o prompt ao chatbot\n",
        "#query = \"Quem é o amor de Bentinho em Dom Casmurro?\"\n",
        "#query = \"Quem é Capitu em Dom Casmurro?\"\n",
        "query = \"Como Bentinho ganhou a alcunha de Dom Casmurro?\"\n",
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f22231c5",
      "metadata": {
        "id": "f22231c5"
      },
      "source": [
        "#### E se eu quiser saber quais textos foram utilizados para o LLM dar a resposta?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694343cb",
      "metadata": {
        "id": "694343cb"
      },
      "outputs": [],
      "source": [
        "# Cria um retriever com a opção \"return_source_documents=True\"\n",
        "qa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                chain_type=\"stuff\",\n",
        "                                retriever=docsearch.as_retriever(),\n",
        "                                return_source_documents=True)\n",
        "#query = \"Quem é o amor de Bentinho em Dom Casmurro?\"\n",
        "#query = \"Quem é Capitu em Dom Casmurro?\"\n",
        "query = \"Como Bentinho ganhou a alcunha de Dom Casmurro?\"\n",
        "result = qa({\"query\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec53323",
      "metadata": {
        "id": "bec53323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9515c0ac-c861-4376-828c-02c8073d6c02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Bentinho foi alcunhado Dom Casmurro por um rapaz que encontrou no trem da Central. O rapaz havia recitado versos para Bentinho, mas como Bentinho estava cansado e fechou os olhos, o rapaz interrompeu a leitura e ficou amuado. Por isso, ele começou a dizer nomes feios de Bentinho e acabou por lhe dar a alcunha de Dom Casmurro.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Mostra resposta do prompt\n",
        "result['result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32246ae3",
      "metadata": {
        "id": "32246ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74949c9b-f52d-427a-fda9-d0d4af93d49d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Romance, Dom Casmurro, 1899\\n\\nDom Casmurro\\n\\nTexto de referência:\\n\\nObras Completas de Machado de Assis, vol. I,\\n\\nNova Aguilar, Rio de\\n\\nJaneiro, 1994.\\n\\nPublicado originalmente pela Editora Garnier, Rio de Janeiro, 1899.\\n\\nCAPÍTULO PRIMEIRO\\n\\nDO TÍTULO\\n\\nUma noite destas, vindo da cidade para o Engenho Novo, encontrei no trem da Central um rapaz aqui do bairro, que eu conheço de vista e de chapéu. Cumprimentou-me, sentou-se ao pé de mim, falou da Lua e dos ministros, e acabou recitando-me versos. A viagem era curta, e os versos pode ser que não fossem inteiramente maus. Sucedeu, porém, que, como eu estava cansado, fechei os olhos três ou quatro vezes; tanto bastou para que ele interrompesse a leitura e metesse os versos no bolso.\\n\\n\\x97 Continue, disse eu acordando.\\n\\n\\x97 Já acabei, murmurou ele.\\n\\n\\x97 São muito bonitos.', metadata={'source': 'machado/marm08.txt'}),\n",
              " Document(page_content=\"Vi-lhe fazer um gesto para tirá-los outra vez do bolso, mas não passou do gesto; estava amuado. No dia seguinte entrou a dizer de mim nomes feios, e acabou alcunhando-me Dom Casmurro. Os vizinhos, que não gostam dos meus hábitos reclusos e calados, deram curso à alcunha, que afinal pegou. Nem por isso me zanguei. Contei a anedota aos amigos da cidade, e eles, por graça, chamam-me assim, alguns em bilhetes: 'Dom Casmurro, domingo vou jantar com você\\x94.\\x97 'Vou para Petrópolis, Dom Casmurro; a casa é a mesma da Renânia; vê se deixas essa caverna do Engenho Novo, e vai lá passar uns quinze dias comigo\\x94.\\x97 'Meu caro Dom Casmurro, não cuide que o dispenso do teatro amanhã; venha e dormirá aqui na cidade; dou-lhe camarote, dou-lhe chá, dou-lhe cama; só não lhe dou moça\\x94.\", metadata={'source': 'machado/marm08.txt'}),\n",
              " Document(page_content='E Rubião explicou aos novatos a alusão ao filósofo, e a razão do nome do cão, que todos lhe atribuíam. Quincas Borba (o defunto) foi descrito e narrado como um dos maiores homens do tempo, \\x97 superior aos seus patrícios. Grande filósofo, grande alma, grande amigo. E no fim, depois de algum silêncio, batendo com os dedos na borda da mesa, Rubião exclamou:\\n\\n\\x97 Eu o faria ministro de Estado!\\n\\nUm dos convivas exclamou, sem convicção, por simples ofício:\\n\\n\\x97 Oh! sem dúvida!\\n\\nNenhum daqueles homens sabia, entretanto, o sacrifício que lhes fazia o Rubião. Recusava jantares, passeios, interrompia conversações aprazíveis, só para correr à casa e jantar com eles. Um dia achou meio de conciliar tudo. Não estando ele em casa às seis horas em ponto, os criados deviam pôr o jantar para os amigos. Houve protestos; não, senhor, esperariam até sete ou oito horas. Um jantar sem ele não tinha graça.\\n\\n\\x97 Mas é que não posso vir, explicou Rubião.', metadata={'source': 'machado/marm07.txt'}),\n",
              " Document(page_content=\"E minha mãe beijava-me com uma ternura que não sei escrever. Tio Cosme, para alegrá-la, chamava-me doutor, José Dias também, e todos em casa, a prima, os escravos, as visitas, Pádua, a filha, e ela mesma repetiam-me o título.\\n\\nCAPÍTULO C\\n\\n'TU SERÁS FELIZ,\\n\\nBENTINHO\\x94\\n\\nNo quarto, desfazendo a mala e tirando a carta de bacharel de dentro da lata, ia pensando na felicidade e na glória. Via o casamento e a carreira ilustre, enquanto José Dias me ajudava, calado e zeloso. Uma fada invisível desceu ali e me disse em voz igualmente macia e cálida: 'Tu serás feliz, Bentinho; tu vais ser feliz.\\x94\\n\\n\\x97 E por que não seria feliz? perguntou José Dias, endireitando o tronco e fitando-me.\\n\\n\\x97 Você ouviu? perguntei eu erguendo-me também, espantado.\\n\\n\\x97 Ouviu o quê?\\n\\n\\x97 Ouviu uma voz que dizia que eu serei feliz?\\n\\n\\x97 É boa! Você mesmo é que está dizendo...\", metadata={'source': 'machado/marm08.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Mostra os textos que foram utilizados na produção da resposta\n",
        "result['source_documents']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pontos importantes\n",
        "\n",
        "\n",
        "*   Provavelmente o chatGPT por si só já conseguiria responder a perguntas sobre as obras de Machado de Assis, visto que foi treinado em muitos textos acerca das obras. Para verificar todo o potencial da solução, **faça testes em uma coleção de documentos privada**, que contenha informações não incluídas na construção dos modelos GPT.\n",
        "*   Este agente conversacional é baseado na recuperação de informações e busca semântica, portanto, **a assertividade do mesmo depende muito do sucesso na seleção de documentos relevantes** que serão inputados no contexto do modelo (i.e., *in-context learning*)\n",
        "* Caso tenha ficado com dúvidas da teoria por trás da busca semântica, acesse [este link](https://txt.cohere.com/what-is-semantic-search/)\n",
        "* O LangChain é uma ferramenta muito completa para construção de agentes conversacionais na era dos LLMs, e possui integração com os mais diversos serviços e LLMs (tente adaptar este código para não utilizar mais os modelos da OpenAI, que tem custo de execução - esta execução não deve gastar mais que alguns centavos de dólar)\n",
        "* Por outro lado, utilizar os serviços da OpenAI pode ser um excelente ponto de partida para construção de um MVP de produto conversacional.\n",
        "\n"
      ],
      "metadata": {
        "id": "zJE9LSkQ1xMy"
      },
      "id": "zJE9LSkQ1xMy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Referências e Material complementar\n",
        "\n",
        "*   [LangChain Youtube Playlist by Greg Kamradt](https://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5&index=1)\n",
        "*   [LangChain Python Documentation](https://python.langchain.com/)\n",
        "* [O que é busca semântica?](https://txt.cohere.com/what-is-semantic-search/)"
      ],
      "metadata": {
        "id": "XJdkc5OA34Ox"
      },
      "id": "XJdkc5OA34Ox"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook foi produzido por Prof. [Lucas Oliveira](http://lattes.cnpq.br/3611246009892500)."
      ],
      "metadata": {
        "id": "q5_PXvmP3zt7"
      },
      "id": "q5_PXvmP3zt7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}