# -*- coding: utf-8 -*-
"""S4-Representação vetorial de textos - Bag of words.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1az1Cgr4EzM0eYECdgSciJmD7midFESoK

># <b>Representação vetorial de textos – *Bag of words*</b>
>Instituição: PUCPR  
>Curso: Tecnologia em Inteligência Artifical Aplicada  
>Disciplina: Processamento de Linguagem Natural  
>Professor: Lucas Oliveira   
>Estudante: Carla Edila Silveira  
> Data: 14/04/2024




## <b>Processamento de Linguagem Natural</b>
Hoje trabalharemos com um assunto essencial ao PLN moderno, a representação vetorial de textos. Nesta aula você realizará atividades práticas relacionadas a técnica chamada  **Bag of words (BoW)**, visando:
1. Entender o modelo Bag of Words
2. Vetorizar textos utilizando a função `CountVectorizer` da biblioteca `scikit-learn`
3. Compreender as limitações do modelo e como ele pode ser utilizado em tarefas de PLN

### **O que é Bag of Words?**

É uma técnica de PLN na qual transformamos textos em **vetores numéricos** para **extrair características do texto**. Tais características podem ser interpretadas por diversos algoritmos, incluindo (principalmente) os de *Machine Learning*.

Apenas dois passos são necessários no algoritmo de BoW:
1.   Determinar o vocabulário do(s) texto(s)
2.   Realizar contagem do termos (frequência das palavras)

Imagine um corpus com os seguintes documentos:

*   *O menino correu*
*   *O menino correu do cão*
*   *O menino com o cão*

#### 1) Determinar vocabulário
Para determinar o vocabulário, basta definirmos uma lista com todas palavras contidas em nosso corpus.
As palavras encontradas nos documentos acima são: `o`, `menino`, `correu`, `do`, `cão` e `com`

#### 2) Contagem das palavras
Nesta etapa devemos contar quantas vezes cada palavra do vocabulário aparece em cada documento/texto, e criamos um vetor com as quantidades computadas.

![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1wDFX5RknqY8eBBaeurIqPK5KgwzjAKfd)

Assim são gerados vetores individuais de cada documento:

*   *O menino correu*: `[1, 1, 1, 0, 0, 0]`
*   *O menino correu do cão*: `[1, 1, 1, 1, 1, 0]`
*   *O menino com o cão*: `[2, 1, 0, 0, 1, 0]`

> **SACO DE PALAVRAS?** A técnica tem esse nome porque se perde toda a informação contextual do texto, ou seja, perde-se onde cada palavra apareceu em cada documento, como se pegássemos todas palavras e colocássemos dentro de um saco!

> A matriz com as frequências das palavras também é chamada de **MATRIZ TERMO-DOCUMENTO**

**Mas, qual a ideia por trás do BoW?**

O BoW segue a ideia de que **documentos semelhantes terão contagens de palavras semelhantes** entre si. Em outras palavras, quanto mais semelhantes forem as palavras em dois documentos, mais semelhantes poderão ser os documentos.
Além disso, ao definir a matriz termo-documento, intui-se que **palavras com alta ocorrência em um documento, sejam importantes a ele**, ou seja, devem estar entre os temas centrais do texto.

**EXEMPLO**: Imagine os vetores a seguir:

![Tabela Bag of Words](https://docs.google.com/uc?export=download&id=1dKga70Q9l3IRtW28TBqTcn8c_0kEQcrS)

O vetor do documento 1 e 2 são similares (assim como seus textos). Já o vetor do documento 3 difere completamente.

### **Utilizando o scikit-learn para calcular o BoW ou matriz termo-documento**
Apesar de ser uma técnica simples de se implementar, não há necessidade, pois ela já é implementada dentro da biblioteca `scikit-learn` sob o nome de `CountVectorizer`
"""

# Exemplo de corpus
corpus = [
            "o menino correu.",
            "o menino correu do cão.",
            "O Menino com o Cão."
]

# corpus = [
#             "gatos são bonitos",
#             "gatos são lindos",
#             "tomate é fruta"
# ]

# Importa funcionalidade de BoW
from sklearn.feature_extraction.text import CountVectorizer

# Cria instância de CountVectorizer
vect = CountVectorizer()

# Transforma o corpus em vetores numéricos (BoW)
X = vect.fit_transform(corpus)
X

# Imprime a ordem de cada coluna
# print(vect.get_feature_names()) - houve erro de execução
print(list(vect.vocabulary_.keys()))

# Imprime vetores (BoW)
print(X.toarray())

"""> **ATENÇÃO**: O CountVectorizer já transforma as palavras em `lowercase` por padrão, ignora pontuação e coloca as palavras em ordem alfabética nos vetores. Além disso ignora palavras que tenham frequência abaixo ou acima dos parâmetros `min_df` e `max_df`.

É possível também **vetorizar N-Grams** do corpus usando o CountVectorizer, sem necessidade de usar alguma função extra. Geralmente o fazemos para obter mais **contexto** do texto.
"""

# Cria instância de CountVectorizer
# Apenas 2-grams serão gerados
vect = CountVectorizer( ngram_range=(2,2) )

# Transforma o corpus em vetores numéricos (BoW)
X = vect.fit_transform(corpus)
X

# Imprime a ordem de cada coluna
# print(vect.get_feature_names()) - houve erro de execução
print(list(vect.vocabulary_.keys()))

# Imprime vetores (BoW)
print(X.toarray())

"""> **DICA**: Vale a pena olhar a [documentação](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do CountVectorizer, pois existem diversos parâmetros úteis que podemos utilizar.

### **Limitações do BoW**
Apesar das ideias de que **documentos semelhantes terão contagens de palavras semelhantes** entre si (ao aprender Classificação de Textos, você poderá ver isso mais detalhadamente) e que **uma palavra com alta frequência em um documento é considerada importante** funcionarem em vários casos, o modelo BoW tem algumas limitações, entre elas:

*   **Peso igual a todas palavras**: o BoW dá um peso igual a todas palavras. Em nosso exemplo palavras como "com" e "do", tem o mesmo peso de "cão" e "menino". Isso não é bom, pois palavras mais comuns (artigos, preposições, etc) deveriam ter peso menor, por serem menos discriminantes.
*   **Significado semântico**: a abordagem básica do BOW não considera o significado da palavra no documento. Ignora completamente o contexto em que é usado. A mesma palavra pode ser usada em vários locais com base no contexto ou nas palavras próximas (embora o uso de n-grams possa amenizar um pouco o problema do contexto).
*   **Tamanho do vetor - maldição da dimensionalidade**: para um documento grande, o tamanho do vetor pode ser enorme, resultando em muito tempo de processamento e alto consumo de memória. Pode ser necessário ignorar as palavras com base na relevância do seu caso de uso.

### **Aplicações do BoW**
É útil em qualquer tarefa em que a posição ou informação contextual do texto não é tão importante. Alguns exemplos são:

*   Identificar o autor de um documento (**classificação de textos**)
*   Agrupar documentos por tópicos (**clusterização**)
*   Análise de sentimentos - identificar "positividade"/"negatividade" de um documento (**regressão**)

### **ATIVIDADE PRÁTICA** - Análise de vetorização de corpus
Nesta atividade, analisaremos um pequeno corpus fictício, realizaremos a vetorização do mesmo, discutiremos problemas no modelo e eventualmente tentaremos corrigí-los.
"""

# Um corpus fictício com 8 documentos
corpus = [
          'Um caminhão está descendo rapidamente um morro.',#0
          'O caminhão está rapidamente descendo o morro.',#1
          'O menininho come bananas.',#2
          'O menino comeu banana.',#3
          'Pizza? Portuguesa!',#4
          'Carro? Marea!',#5
          'Qual Seu Nome?',#6
          'qual seu nome?'#7
]

# Código da QUESTÃO 4
# Inicializar o objeto CountVectorizer
vect = CountVectorizer()

# Ajustar e transformar o corpus
vetorizado = vect.fit_transform(corpus)

# Obtém as palavras presentes no vocabulário
palavras = vect.get_feature_names_out()

# Imprime as palavras presentes no vocabulário
print(palavras)

"""#### 1) Mostre a quantidade de caracteres e tokens do corpus


> **DICA**: É possível concatenar todas sentenças da lista em uma String apenas usando a função `join()`


"""

# Concatenar todas as sentenças em uma única string
corpus_completo = ' '.join(corpus)

# Contar o número de caracteres na string
quantidade_caracteres = len(corpus_completo)

# Contar o número de tokens (palavras) na string
quantidade_tokens = len(corpus_completo.split())

# Imprimir a quantidade de caracteres e tokens
print("Quantidade de caracteres no corpus:", quantidade_caracteres)
print("Quantidade de tokens no corpus:", quantidade_tokens)

"""#### 2) Mostre a quantidade de tokens únicos do corpus
Ao saber a quantidade de tokens únicos, qual informação importante você pode afirmar a respeito de um possível BoW?
"""

# Criar um conjunto de tokens únicos
tokens_unicos = set(corpus_completo.split())

# Contar o número de tokens únicos
quantidade_tokens_unicos = len(tokens_unicos)

# Imprimir a quantidade de tokens únicos
print("Quantidade de tokens únicos no corpus:", quantidade_tokens_unicos)

"""OBSERVAÇÃO: Saber a quantidade de tokens únicos no corpus é importante para entender a diversidade vocabular do corpus. Isso pode indicar o tamanho do vocabulário que será usado na construção de um modelo Bag of Words (BoW). Quanto maior o número de tokens únicos, mais rico e variado é o vocabulário do corpus. Isso também pode influenciar o tamanho e a complexidade do vetor de características resultante do modelo BoW.

#### 3) Aplique o BoW no corpus e mostre as dimensões, palavras e os valores do vetor resultante
"""

# Inicializar o objeto CountVectorizer
vect = CountVectorizer()

# Ajustar e transformar o corpus
vetorizado = vect.fit_transform(corpus)

# Obter as palavras presentes no vocabulário
palavras = vect.get_feature_names_out()

# Obter a matriz de características (vetor resultante)
matriz_caracteristicas = vetorizado.toarray()

# Imprimir as dimensões da matriz de características
print("Dimensões da matriz de características:", matriz_caracteristicas.shape)

# Imprimir as palavras presentes no vocabulário
print("Palavras presentes no vocabulário:")
print(palavras)

# Imprimir os valores do vetor resultante para cada documento
print("Valores do vetor resultante:")
for i, doc in enumerate(corpus):
    print("Documento", i, ":", matriz_caracteristicas[i])

"""#### 4) Coloque o nome das colunas e valores do vetor em um `DataFrame` do `pandas`"""

import pandas as pd
# Criar um DataFrame com os valores do vetor resultante
df = pd.DataFrame(matriz_caracteristicas, columns=palavras)

# Imprimir o DataFrame
print(df)

"""#### 5) Ao comparar os vetores resultantes e aplicar a função de Distância de Jaccard nos documentos 2 e 3 verificamos que eles são bem dissimilares, apesar de serem sentenças com valor semântico similar. **Como podemos tornar os vetores do BoW para estes documentos mais semelhantes?**

OBSERVAÇÕES: Se os vetores resultantes do modelo Bag of Words (BoW) para os documentos 2 e 3 são bem dissimilares, apesar de serem sentenças com valor semântico similar, isso pode indicar que o BoW está falhando em capturar a semelhança semântica entre esses documentos. Para tornar os vetores do BoW mais semelhantes, podemos considerar algumas abordagens:

1. **Pré-processamento de texto**: Antes de aplicar o BoW, podemos realizar pré-processamento de texto para reduzir as variações nos documentos, como remover stop words, aplicar stemming ou lematização e converter todas as palavras para minúsculas. Isso pode ajudar a aumentar a similaridade entre os documentos.

2. **Ajustar parâmetros do CountVectorizer**: Podemos ajustar os parâmetros do CountVectorizer para considerar diferentes aspectos dos documentos, como o uso de n-gramas, a remoção de palavras raras ou muito comuns, e o uso de tokenização personalizada. Experimentar diferentes configurações pode levar a vetores mais semelhantes entre documentos semanticamente similares.

3. **Uso de modelos de incorporação de palavras**: Em vez de usar o BoW, podemos considerar o uso de modelos de incorporação de palavras (word embeddings), como Word2Vec, GloVe ou FastText. Esses modelos capturam melhor a semântica das palavras e podem fornecer representações vetoriais mais ricas e semânticas dos documentos.

4. **Combinação de características**: Podemos combinar os vetores do BoW com outras características, como contagens de bigramas, trigramas, TF-IDF ou características baseadas em semântica latente, para capturar melhor a semelhança semântica entre os documentos.

5. **Uso de outras medidas de similaridade**: Além da Distância de Jaccard, podemos explorar outras medidas de similaridade, como a similaridade de cosseno, similaridade de Jaccard ponderada, similaridade de Jaccard baseada em TF-IDF, etc. Algumas dessas medidas podem ser mais adequadas para capturar a semelhança semântica entre os documentos.

Ao experimentar essas abordagens e ajustar os parâmetros do modelo, podemos tentar melhorar a capacidade do BoW em capturar a semelhança semântica entre os documentos, tornando os vetores resultantes mais semelhantes para documentos semanticamente similares.
"""

# Importa bibliotecas para pré-processamento
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Inicializar o objeto CountVectorizer com opções de pré-processamento
stop_words = stopwords.words('portuguese')  # Obter lista de stop words em português
stemmer = PorterStemmer()  # Inicializar o stemmer
vect = CountVectorizer(lowercase=True, stop_words=stop_words)

# Realizar pré-processamento de texto para cada documento
corpus_preprocessado = []
for doc in corpus:
    # Converter todas as palavras para minúsculas
    doc = doc.lower()
    # Tokenizar o documento
    tokens = word_tokenize(doc)
    # Remover stop words
    tokens = [token for token in tokens if token not in stop_words]
    # Aplicar stemming
    tokens = [stemmer.stem(token) for token in tokens]
    # Juntar os tokens de volta em uma string
    doc_preprocessado = ' '.join(tokens)
    # Adicionar o documento pré-processado à lista
    corpus_preprocessado.append(doc_preprocessado)

# Ajustar e transformar o corpus pré-processado
vetorizado = vect.fit_transform(corpus_preprocessado)

# Obter as palavras presentes no vocabulário
palavras = vect.get_feature_names_out()

# Obter a matriz de características (vetor resultante)
matriz_caracteristicas = vetorizado.toarray()



# Imprimir as dimensões da matriz de características
print("Dimensões da matriz de características:", matriz_caracteristicas.shape)

# Imprimir as palavras presentes no vocabulário
print("Palavras presentes no vocabulário:")
print(palavras)

# Imprimir os valores do vetor resultante para cada documento
print("Valores do vetor resultante:")
for i, doc in enumerate(corpus_preprocessado):
    print("Documento", i, ":", matriz_caracteristicas[i])

# Criar um DataFrame com os valores do vetor resultante
df = pd.DataFrame(matriz_caracteristicas, columns=palavras)

# Imprimir o DataFrame
print(df)

"""Você pode alterar/incluir células nas questões anteriores para resolver essa questão. Depois re-execute as 3 células acima para checar os resultados.

#### 6) Gere um BoW com 2-grams e 3-grams
Olhe a documentação do CountVectorizer para verificar como parametrizar para gerar os dois simultaneamente.
"""

# Inicializar o objeto CountVectorizer para extrair 2-grams e 3-grams
vect = CountVectorizer(ngram_range=(2, 3))

# Ajustar e transformar o corpus
vetorizado = vect.fit_transform(corpus)

# Obter as palavras presentes no vocabulário
palavras = vect.get_feature_names_out()

# Obter a matriz de características (vetor resultante)
matriz_caracteristicas = vetorizado.toarray()

# Imprimir as dimensões da matriz de características
print("Dimensões da matriz de características:", matriz_caracteristicas.shape)

# Imprimir as palavras presentes no vocabulário
print("Palavras presentes no vocabulário:")
print(palavras)

# Imprimir os valores do vetor resultante para cada documento
print("Valores do vetor resultante:")
for i, doc in enumerate(corpus):
    print("Documento", i, ":", matriz_caracteristicas[i])

"""O gabarito para as atividades encontra-se [aqui](https://colab.research.google.com/drive/1-w8fIbyRnj9oPB03WNWBZgp-4eqcUhBr).

## Referências e Material complementar

*   [An Introduction to Bag-of-Words in NLP](https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)
*   [A Simple Explanation of the Bag-of-Words Model](https://victorzhou.com/blog/bag-of-words/)
"""